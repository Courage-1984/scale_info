# The Science and Application of Native Resolution Restoration in Digital Media: A Comprehensive Analysis of Upscaling Inversion

## Executive Summary

The digital media landscape is increasingly populated by content that has undergone various forms of upscaling, particularly older productions re-released to conform to contemporary hardware standards, such as Blu-ray re-releases of standard definition anime or UHD Blu-ray versions of previously lower-resolution animation and live-action movies. This widespread practice has given rise to the specialized field of native resolution restoration, commonly known as descaling, which involves the precise reversal of these upscaling operations.

### Core Benefits of Native Resolution Restoration

**File Size and Bandwidth Optimization**: Descaling leads to substantial reductions in file size and associated bandwidth consumption during distribution. An upscaled 1080p video from an original 720p source, despite its larger size, contains no additional information. This efficiency extends to long-term archival storage, where conserving hard drive space is a critical concern.

**Processing Efficiency**: Processing and compression tasks become significantly more efficient at native resolutions, as the lower pixel count per frame reduces computational burdens and processing time for video filters and compression techniques.

**Quality Enhancement Through Strategic Re-upscaling**: Descaling enables the subsequent re-upscaling of content using more sophisticated and higher-quality algorithms, which can yield a subjectively superior high-resolution image with fewer artifacts like haloing or ringing, compared to simply applying standard resizing methods directly to an already upscaled source. This two-stage process, where a "cheap" initial upscale is undone to allow for a superior re-upscale, often results in a marked improvement in perceived visual quality for the end-user.

### Detection Methodologies

Two principal methodologies dominate the detection of native resolutions:

**Fourier Transform Analysis**: Often implemented by tools like resdet, this method identifies unique frequency domain signatures, such as "zero-crossings," that are characteristic of traditional upscaling operations and directly correspond to the original resolution.

**Error Measurement Method**: Exemplified by tools such as getnative and GetFnative, this employs an iterative trial-and-error approach where the image is downscaled to various candidate resolutions, then re-upscaled, and the absolute difference (error) from the original is calculated. A significant dip in this error indicates the true native resolution.

### Software Ecosystem

The practical application of descaling relies heavily on a robust software ecosystem. The Vapoursynth frame server serves as a foundational platform, supporting specialized plugins like Descale which perform the actual inverse resizing. Complementary Python scripts, including getnative, resdet, getfscaler, and GetFnative, automate the detection process, analyze image characteristics, and facilitate the application of descaling operations.

### Challenges and Limitations

Despite its considerable benefits, native resolution restoration presents inherent complexities and limitations:

- **Kernel Identification**: Accurately identifying the original resize kernel and its parameters
- **Mixed Resolutions**: Dealing with content that exhibits mixed resolutions (e.g., native elements alongside upscaled graphics within the same frame)
- **Compression Artifacts**: Mitigating the detrimental impact of compression artifacts and pre-existing noise on detection accuracy
- **Neural Network Limitations**: The inability of deterministic descaling methods to reverse upscales generated by advanced neural network-based algorithms, which synthesize new details rather than merely interpolating existing ones

### TL;DR Summary

- Descaling is a well-understood, mostly reversible operation for deterministic resizers (bilinear, bicubic, etc.)
- Two complementary detection approaches exist: frequency/Fourier analysis and error-metric/trial-descale methods
- Practical tooling is mature: resdet (FFT/spectral), getnative (error metric scripts), GetFnative/getfscaler (fractional detection), and the descale plugin
- Major caveats: overlays (credits/text), studio postprocessing, mixed-resolution layers, heavy lossy compression, and neural upscalers break reversibility or detection
- Always validate visually and with an error metric

## 1. Introduction to Digital Image Resizing and Native Resolution

### 1.1 Defining Image Resizing and Scaling: Upscaling vs. Downscaling

Image resizing, often interchangeably referred to as scaling, constitutes a fundamental operation within digital image processing. Its core function is to systematically alter the dimensions of a given image or video frame without fundamentally distorting its overall structure or the integrity of its content. This process is achieved through sophisticated algorithms that either interpolate new pixel data when increasing size or sample existing data when decreasing size.

#### Upscaling (Magnification)

The operation of increasing an image's dimensions is termed upscaling, or magnification. During upscaling, the computational task involves generating new pixels to fill the expanded grid. These new pixel values are not derived from novel information but are instead estimated or interpolated based on the values of the existing, original pixels.

A critical implication of this interpolation-based approach is that upscaling does not genuinely introduce new information or fine detail that was not present in the original source. Instead, it effectively spreads the existing information across a larger number of pixels. This often results in a perceived softening of the image or, if performed suboptimally, the introduction of visual artifacts such as aliasing or ringing.

The understanding that upscaling is primarily an interpolative, rather than generative, process is foundational to the feasibility and efficacy of descaling. If upscaling were to genuinely create unique, uninferable information, the inverse process of descaling would be rendered impossible.

#### Downscaling (Minification)

Conversely, downscaling, or minification, involves reducing the resolution or dimensions of an image. This process typically necessitates the selective discarding of pixel information and the averaging of existing data to fit the smaller grid. While downscaling can be optimized to preserve detail, it inherently involves a loss of some fine information due to pixels being consolidated.

The fundamental distinction between upscaling and downscaling, particularly the interpolative nature of the former, is paramount for comprehending the rationale behind descaling. Descaling leverages the deterministic nature of many upscaling algorithms to reverse the interpolation, thereby recovering the original pixel values as accurately as possible.

### 1.2 The Concept of "Native Resolution" in Digital Media Production

The term "native resolution" holds a precise and critical meaning within the domain of digital media. It is defined as the original resolution at which any given material was initially produced. This encompasses the diverse array of content creation methodologies, whether the material was originally captured through filming, scanned from physical media, meticulously drawn, animated frame-by-frame, or generated through other digital processes. The native resolution, therefore, represents the true, intrinsic resolution of the source data before any subsequent scaling operations are applied.

#### Common Native Resolutions in Anime

For anime content, common native resolutions include:

- **720p** (1280x720): Standard definition, often used for TV series
- **810p** (1440x810): Intermediate resolution
- **846p** (1504x846): Common in certain studio workflows
- **864p** (1536x864): Another intermediate resolution
- **900p** (1600x900): Higher quality intermediate
- **955.5p**: Specific to Kyoto Animation workflows (arises from upscaling to 1088p then cropping to 1080p)

#### Illustration of Native Resolution Concept

To illustrate this concept, consider a common scenario in video distribution: a video file that was originally filmed or animated in 720p (1280x720 pixels) but was subsequently upscaled to 1080p (1920x1080 pixels) for its Blu-ray release. In this specific instance, while the final distributed file possesses a resolution of 1080p, its native resolution unequivocally remains 720p. This distinction is not merely semantic; it is crucial for understanding the core purpose and technical objectives of descaling. The goal of descaling is to revert the content to this original, pre-upscale state.

#### Mixed Resolution Scenarios

A nuanced aspect of native resolution arises in the context of synthetic media, such as computer-generated imagery (CGI). In such productions, it is technically possible for a single frame to comprise elements that were produced at multiple distinct native resolutions. For example, a background might be rendered at a higher resolution, while foreground characters or specific visual effects might be generated at a lower resolution and then composited and upscaled to match the overall frame.

This inherent complexity in synthetic media directly foreshadows the challenges associated with descaling content that contains mixed resolutions, a topic that warrants detailed discussion in later sections. The ability to identify and address these varying native resolutions within a single frame is a hallmark of advanced descaling techniques.

### 1.3 The Imperative for Native Resolution Restoration: Benefits and Use Cases

The restoration of an image or video to its original native resolution, a process known as descaling, offers a compelling array of benefits that extend across various stages of the digital media lifecycle, from production and distribution to archival and end-user consumption.

#### Economic and Storage Benefits

A primary driver for descaling is the significant reduction in file size and the consequent improvement in bandwidth efficiency. An upscaled video file, such as a 1080p version derived from an original 720p source, is inherently and substantially larger than its native-resolution counterpart. This increase in size occurs despite the critical fact that the upscaled file contains no genuinely new or additional information beyond what was present in the original native resolution.

This fundamental characteristic means that the extra data in an upscaled file represents redundant or interpolated information. By restoring content to its native resolution, file sizes are directly and often dramatically decreased. This translates into considerable advantages for efficient digital distribution, leading to reduced bandwidth costs for streaming services and faster download times for end-users.

For long-term archival storage, conserving expensive hard drive space is a paramount concern, and descaling offers a direct solution to optimize storage infrastructure. The recurring emphasis on "wasted bandwidth" and "hard drive space" in discussions of descaling underscores its role not merely as a technical refinement but as a substantial economic and resource optimization strategy. For large-scale content distributors or archival institutions, even marginal percentage savings per file, when scaled across vast libraries, translate into immense cost reductions in storage infrastructure, data transfer fees, and computational power.

#### Processing and Workflow Optimization

Beyond storage and bandwidth, descaling critically optimizes processing and compression workflows. The higher pixel count per frame in upscaled media directly translates to increased computational demands. Tasks such as applying complex video filters, performing precise color corrections, or executing various compression algorithms become significantly more time-consuming and resource-intensive when operating on upscaled content.

By reverting to the native resolution, these computational burdens are substantially reduced, leading to faster and more efficient encoding, post-production, and transcoding workflows.

#### Quality Enhancement Through Strategic Re-upscaling

Perhaps one of the most compelling benefits, particularly for visual quality, is the ability to achieve enhanced perceived image quality through strategic re-upscaling. Publishers and digital media distributors frequently employ "cheap" or suboptimal upscaling techniques to adapt older content or lower-resolution productions to current hardware standards, such as 1080p or 4K Blu-ray releases. This practice, while meeting market demands, often introduces undesirable artifacts like haloing or ringing.

A key advantage of descaling is its capacity to reverse these initial, often inferior, upscales. Once the content is restored to its pristine native resolution, it can then be re-upscaled using a more sophisticated and higher-quality algorithm. This two-step process can result in a subjectively superior high-resolution image, often appearing sharper with fewer artifacts compared to simply applying standard resizing methods like Spline36 directly to an already upscaled source.

This indicates that descaling is frequently not an end in itself, but a strategic intermediate step. The primary objective often extends beyond merely returning to the native resolution; it is a tactical maneuver to "cleanse" the image of artifacts introduced by a poor initial upscale, thereby providing a pristine native-resolution base. This clean base can then be re-upscaled using a superior algorithm, either by the encoder or the end-user's playback system, which can achieve a higher perceived quality than the original upscaled release.

This implies a powerful two-stage optimization: first, undo the damage; second, apply a better transformation.

#### Archival and Preservation Benefits

Finally, for preservationists and archivists, restoring the original native state of content is paramount for maintaining archival integrity and fidelity to the original source. It ensures that digital media is stored and processed as close as possible to its true origin, preserving its intrinsic quality, especially in environments where strict adherence to current hardware standards is not the primary concern.

The very existence and evolution of descaling tools and techniques within enthusiast and archival communities is a direct, community-driven response to this industry-level quality compromise, aiming to rectify the visual artifacts and inefficiencies introduced by these "cheap" upscales. This highlights a dynamic tension between commercial expediency and the pursuit of content perfection.

## 2. Theoretical Foundations of Image Resizing Kernels and Algorithms

### 2.1 Principles of Traditional Resizers

The fundamental principle underpinning all traditional, non-neural network-based image resizers is the calculation of new pixel values as a weighted average of a selected number of surrounding "reference pixels". The specific count of these reference pixels, denoted as 'n', varies depending on the particular kernel employed, directly influencing the complexity and visual characteristics of the resizing operation.

#### Common Resizing Kernels

A range of common kernels is utilized in traditional image resizing, each with distinct properties and applications:

**Point or Nearest Neighbor**: This is the simplest and fastest resizing method, characterized by n=1 reference pixel. The value of a new pixel is directly assigned from the single closest existing pixel in the source image. While computationally efficient, this method often results in blocky, jagged, or aliased artifacts, particularly noticeable during upscaling, as it lacks any form of interpolation to smooth transitions.

**Linear**: Utilizing n=2 reference pixels, the Linear kernel performs a basic linear interpolation between adjacent pixels. This method offers a slight improvement over Nearest Neighbor by introducing some smoothing, but it still produces relatively soft or blurry results, especially when scaling significantly.

**Bilinear**: This kernel involves n=4 reference pixels. It performs linear interpolation sequentially, first in the horizontal (x) direction and then in the vertical (y) direction. Bilinear interpolation produces a smoother image than Nearest Neighbor, effectively reducing jagged edges, but the resulting image can still appear somewhat soft due to the averaging nature of the interpolation. It is a common choice for quick, general-purpose scaling.

**Cubic**: Also employing n=4 reference pixels, the Cubic kernel distinguishes itself by using cubic curves, such as Lagrange polynomials, for interpolation. This allows for a more sophisticated estimation of pixel values, often resulting in sharper outputs than Bilinear interpolation while maintaining a good degree of smoothness.

**Bicubic**: Representing a more advanced form of interpolation, the Bicubic kernel utilizes n=16 reference pixels. It applies four separate cubic curves for interpolation, providing a high degree of control over the balance between sharpness and smoothness. Bicubic kernels are often parameterized by 'b' and 'c' values, which define the shape of the cubic function and thus influence the specific characteristics of the output, such as sharpness or the presence of ringing artifacts.

Common bicubic variants include:

- **Mitchell-Netravali** (often b=1/3, c=1/3): Balanced sharpness and smoothness
- **Catmull-Rom** (b=0, c=0.5): Sharp interpolation
- **B-Spline** (b=1, c=0): Smooth interpolation
- **Spline36**: High-quality spline-based interpolation

**Lanczos and Spline**: These are other widely used high-quality interpolation kernels, particularly favored for their ability to produce sharp results while effectively minimizing aliasing artifacts. Lanczos, for instance, uses a sinc function and is parameterized by 'taps', which determines the number of samples used for interpolation. Spline kernels, such as Spline16 and Spline36, are known for their smooth transitions and good detail preservation.

#### Mathematical Properties and Determinism

The mathematical properties of these kernels dictate their impact on image quality. A crucial characteristic is their deterministic nature: given the same input image and parameters, these traditional resizers will consistently produce the identical output. This predictability is the cornerstone of their theoretical reversibility, provided that no information was irrecoverably lost during the original upscaling process.

Different kernels inherently introduce distinct visual characteristics, such as varying levels of sharpness, the presence of ringing (undesirable oscillations or halos around sharp edges), or haloing (light or dark borders around high-contrast areas) during upscaling. The objective of descaling is precisely to identify these specific artifactual signatures and reverse them, thereby recovering the original, cleaner image data.

#### Kernel Reference Table

| Kernel                            | n (Reference Pixels)  | Comments                                                                                                   |
| --------------------------------- | --------------------- | ---------------------------------------------------------------------------------------------------------- |
| Linear                            | 2                     | Simple linear interpolation                                                                                |
| Cubic                             | 4                     | Uses curves (e.g., Lagrange)                                                                               |
| Bilinear                          | 4                     | Linear in x and y direction                                                                                |
| Bicubic                           | 16                    | Uses 4 separate cubic curves, parameterized by b/c values                                                  |
| Point/Nearest Neighbor            | 1                     | Simplest, fastest; often results in blocky output                                                          |
| Lanczos                           | Varies (e.g., taps=3) | Known for sharpness and anti-aliasing; uses sinc function                                                  |
| Spline (e.g., Spline16, Spline36) | Varies                | Smooth transitions, good detail preservation; often used for anime upscales, can introduce ringing/haloing |

This table serves as a foundational reference, consolidating crucial information about various deterministic kernels. Understanding the characteristics of these kernels is indispensable for the practical step of accurately identifying the original upscale kernel during the descaling process, as each kernel leaves a distinct signature that can be analyzed and reversed.

### 2.2 Deterministic vs. Neural Network Resizers

The landscape of image resizing algorithms is broadly divided into two major categories: deterministic resizers and neural network-based resizers. These two paradigms operate on fundamentally different principles, leading to significant implications for their predictability, output characteristics, and, crucially, their reversibility.

#### Characteristics and Predictability of Deterministic Algorithms

As previously detailed, deterministic resizers, such as Linear, Bilinear, Cubic, and Bicubic, are predicated on fixed, explicit mathematical formulas. This reliance on predefined mathematical operations ensures their predictable and consistent behavior: given the exact same input image and parameters, these algorithms will invariably produce the identical output. This inherent determinism is the cornerstone of their theoretical reversibility.

The process of inverting such a resize operation can be precisely modeled as solving a linear equation, A∗x=b, where 'A' represents the known resize kernel (encapsulating its mathematical properties and parameters), 'x' is the unknown vector of original pixel values that one aims to recover, and 'b' is the known vector of upscaled pixel values (the input image data).

This mathematical elegance and robustness, however, encounter practical constraints in real-world scenarios. The critical caveat "as long as no information was lost" is paramount. In practice, factors such as lossy compression (e.g., from video codecs or JPEG files), subsequent filtering, and the introduction of noise inevitably introduce irreversible alterations to the pixel data. Therefore, while the mathematical framework for inversion is theoretically pure, its application in practice is constrained by the imperfections of real-world source material. This is why descaling is consistently described as "near-lossless" rather than perfectly lossless.

The theoretical purity of deterministic inversion is always compromised by practical data degradation, transforming the problem from a simple inverse calculation into a sophisticated optimization challenge that seeks the most probable original state rather than the exact one.

#### Overview of Neural Network-Based Upscaling

In contrast, neural network-based upscaling represents a more recent and rapidly evolving frontier in image processing. Prominent examples include waifu2x, NNEDI3, and Google's RAISR. These methods typically involve an initial resize, often performed using a traditional resizer, to bring the image to a target resolution. Following this, a series of multiple convolutional layers or similar filters are applied to each pixel.

The overarching objective of these neural networks is to achieve "super-resolution" with exceptionally high sharpness and fine detail, often by "hallucinating" or synthesizing new details based on complex patterns learned from vast training datasets. This approach fundamentally differs from traditional interpolation.

Traditional methods are fundamentally about interpolation – estimating values between known data points. Neural networks, however, engage in hallucination or synthesis – generating new, plausible details based on learned patterns, effectively creating "information" that was not explicitly present in the original source. This generative aspect makes their inversion fundamentally different and generally intractable with current deterministic methods, as there is no direct mathematical inverse for "imagined" data.

This explains why descaling techniques are primarily applicable to content upscaled using traditional, deterministic algorithms.

#### Challenges of Inverting Neural Network Upscales

The generative and non-linear nature of most neural network upscalers poses significant challenges to their inversion. Unlike deterministic resizers, these algorithms produce "seemingly arbitrary outputs" that are highly dependent on the specific data used to train their underlying models. Their ability to synthesize details not present in the original source, effectively "imagining" new information, makes them generally non-deterministically reversible.

Tools like resdet explicitly acknowledge this limitation, stating their inability to work with "newer neural network-based resizers" due to this fundamental difference in operation. The process of "de-hallucinating" synthesized information, or finding a unique inverse for a generative process, is currently intractable with the inverse mathematical models used for deterministic upscales.

#### Special Cases: NNEDI3 Inversion

Within the realm of neural network-based upscalers, NNEDI3 stands out as a unique exception with a degree of reversibility. It operates by interpolating only every other pixel in an image, crucially leaving the original reference pixels untouched. Due to this highly specific and structured interpolation pattern, its upsampling effect can be reversed relatively simply. This is achieved by removing every other row of pixels in the image, particularly effective for magnifications that are powers of 2 (i.e., 2^n for n∈ℕ).

This makes NNEDI3 a rare instance of a partially reversible neural network upscaler, providing a conceptual bridge between purely deterministic and fully generative AI models. Its specific reversibility suggests that not all "neural network" upscalers are designed with the same level of generative complexity. NNEDI3's interpolation method is highly structured and predictable, making it amenable to a direct inverse operation.

This implies that future, more "explainable" or structured AI upscaling architectures might also possess some degree of reversibility, contrasting with the general difficulty of inverting more complex, black-box generative models.

## 3. The Mathematical Basis of Inverse Resizing (Descaling)

The ability to accurately reverse a deterministic resizing operation, known as descaling, is rooted in the principles of linear algebra. The process of resampling, whether upscaling or downscaling, can be precisely modeled as a linear equation. Understanding this mathematical framework is essential for appreciating the precision and limitations of descaling tools.

### 3.1 The Linear Equation Model: A∗x=b

The fundamental representation of resampling operations is expressed through the linear algebraic model: A∗x=b. This equation serves as the bedrock for understanding how deterministic image processing functions and, critically, how it can be inverted.

#### Components of the Linear Equation

The components of this equation are defined as follows:

**A (Resize Kernel Matrix)**: This is an n×m matrix that encapsulates the specific mathematical properties of the resize kernel that was originally used (e.g., Bicubic, Bilinear) and its associated parameters. In this context, 'm' represents the total number of pixels in the original, native-resolution image, while 'n' represents the total number of pixels in the upscaled (output) image. For upscaling operations, 'n' is typically greater than 'm', indicating that the output image has more pixels than the input. The matrix 'A' effectively describes how each pixel in the original image contributes to the formation of each pixel in the upscaled image, based on the interpolation function of the chosen kernel.

**x (Original Pixels Vector)**: This is a vector containing the pixel values of the original, native-resolution image. It comprises 'm' elements, corresponding to the 'm' pixels of the source image. In the inverse problem of descaling, 'x' is the unknown variable that the process aims to recover. It represents the pristine, un-upscaled data.

**b (Upscaled Pixels Vector)**: This is a vector containing the pixel values of the final, upscaled image. It consists of 'n' elements, corresponding to the 'n' pixels of the target image. This vector is considered known, as it constitutes the input image data that the descaling process receives. It is the observable, upscaled artifact from which the original 'x' must be inferred.

#### The Challenge of Solving the Inverse Problem

The challenge in descaling lies in solving this equation for 'x'. Directly solving for 'x' by inverting 'A' (i.e., x=A⁻¹b) is generally not straightforward or even possible. This is because 'A' is typically a rectangular matrix (not square) when 'n' (output pixels) is not equal to 'm' (input pixels), and thus it does not possess a direct inverse in the traditional sense. Even if 'A' were square, its potentially very large size and its often ill-conditioned nature in real-world scenarios (due to noise, compression, or near-linear dependencies) would pose significant computational challenges for a direct inversion. Therefore, specialized numerical methods are required to find the most accurate approximation of 'x'.

### 3.2 Solving the Inverse Problem

To overcome the inherent difficulties of directly inverting the rectangular or ill-conditioned matrix 'A' in the equation A∗x=b, the problem is transformed into a solvable form using techniques from numerical linear algebra.

#### Transformation to A^T A x = A^T b (Normal Equations)

The standard approach to find the least-squares solution for 'x' in an overdetermined system (where there are more equations than unknowns, as is the case in upscaling where n>m) is to multiply both sides of the equation A∗x=b by the transpose of A, denoted as A^T or A'. This yields the transformed equation: A^T A x = A^T b.

This transformation is a common technique used to convert a system of linear equations into a form where a unique or best-fit solution can be found. The resulting matrix A^T A is now a square, symmetric m×m matrix. Crucially, for well-behaved resizing kernels, this matrix is typically positive-definite, which ensures its invertibility and stability for numerical methods. The vector A^T b is a vector with 'm' elements. This transformed system, known as the normal equations, is now amenable to efficient numerical solution methods, even for very large image dimensions.

#### Detailed Explanation of LDL^T Decomposition

The Descale plugin, a key tool in Vapoursynth for inverse scaling, leverages LDL^T decomposition to solve the system A^T A x = A^T b. LDL^T decomposition is a specific method for factoring a symmetric matrix (like A^T A) into the product of a lower triangular matrix (L), a diagonal matrix (D), and the transpose of the lower triangular matrix (L^T). The decomposition is expressed as A^T A = LDL^T.

Both L and D are triangular matrices, meaning many of their elements are zero. This structure makes subsequent computational steps significantly simpler and more efficient. This decomposition is particularly well-suited for large, sparse, and banded symmetric matrices, which is characteristic of the A^T A matrix derived from image resizing kernels. The sparseness arises because each output pixel only depends on a limited number of input pixels, leading to many zero entries in the 'A' matrix. The "banded" nature means non-zero elements are concentrated around the main diagonal, further aiding computational efficiency.

#### Forward and Back Substitution for Pixel Restoration

Once the LDL^T decomposition of A^T A is performed, the solution for 'x' (the original pixel values) is obtained through a two-step substitution process, effectively breaking down the complex matrix inversion into simpler, sequential solves:

1. **Forward Substitution**: The first step involves solving the equation LDy = A^T b for an intermediate vector 'y'. Since L is a lower triangular matrix and D is a diagonal matrix, this system can be solved efficiently by iterating through the equations from top to bottom, substituting already computed values of 'y' into subsequent equations. This process is computationally straightforward.

2. **Back Substitution**: The final step involves solving the equation L^T x = y for 'x'. Since L^T is an upper triangular matrix (the transpose of a lower triangular matrix), this system can be solved efficiently by iterating through the equations from bottom to top, substituting already computed values of 'x' into preceding equations. This process yields the desired vector 'x', which represents the restored pixel values of the original, native-resolution image.

This entire mathematical process, from formulating the normal equations to applying LDL^T decomposition and subsequent substitutions, allows for a robust and computationally feasible method to invert deterministic upscaling operations, thereby restoring the original image data as accurately as possible within the constraints of real-world data imperfections.

## 4. Methodologies for Native Resolution Detection

Accurately identifying the native resolution of an upscaled image or video is the crucial first step in the descaling process. Two primary methodologies are employed for this purpose: Fourier Transform analysis and the Error Measurement method. Each offers distinct advantages and is suited to different scenarios and data characteristics. The effective application of these methods often benefits from a deep understanding of their underlying principles and limitations.

### 4.1 Fourier Transform Analysis

Fourier Transform analysis, a powerful technique rooted in signal processing, involves transforming an image from its spatial domain (where pixels are represented by their location) into the frequency domain. This transformation, often achieved using the Discrete Cosine Transform (DCT) in image processing contexts, allows for the analysis of the image's constituent frequencies. In the frequency domain, different image components are represented by their corresponding frequencies: high frequencies correspond to very small image components and fine details, such as sharp edges, intricate textures, or subtle noise. Conversely, mid to low frequencies represent the basic image structure, broader features, and smoother gradients.

#### Frequency Domain Signatures of Upscaling

A key indicator of an upscaled image, particularly one processed with traditional deterministic resampling methods, is a noticeable absence or significant attenuation of high-frequency information. This phenomenon occurs because upscaling, being an interpolation process, does not genuinely create new fine details; it merely spreads the existing information across a larger pixel grid. Consequently, the high-frequency content, which represents these fine details, is either smoothed out or simply not generated.

Tools like resdet leverage this principle by specifically analyzing the frequency domain for unique patterns indicative of upscaling. Traditional resampling methods, such as those used by tools like ImageMagick, introduce characteristic "zero-crossings" at specific offsets within the frequency domain. These zero-crossings act as a distinct signature, directly corresponding to the original resolution from which the image was upscaled.

resdet identifies these specific inversions in the frequency spectrum to make its "best guess" about the original resolution. This method has a well-established history within certain online communities, having been utilized for over a decade, particularly for identifying upscales in Japanese animation.

#### Implementation and Tools

**DCT (Discrete Cosine Transform)**: Python-based scripts process frames to reveal grid-like noise patterns indicating original resolution (e.g., 960×540 → 1920×1080 scale). However, inconsistent color depth replication between Python and Vapoursynth implementations can reduce accuracy.

**FFT (Fast Fourier Transform)**: Tools like FFTSpectrum (AviSynth/Vapoursynth) visualize frequency distribution. Native resolutions show as distinct "gaps" in the spectrum where high-frequency data is absent. This approach is effective for fixed-resolution assets (e.g., pixel art games like Shovel Knight) but struggles with mixed-content anime.

#### Limitations and Optimal Conditions

Despite its speed and ability to identify specific signatures, Fourier Transform analysis, particularly as implemented in resdet, has certain limitations and optimal conditions for performance:

- **Source Quality Requirements**: The tool performs optimally on clear, highly detailed images that are as close as possible to the original source
- **Compression Sensitivity**: Its accuracy can be significantly compromised by the presence of compression artifacts (e.g., from highly compressed JPEGs or video codecs) and subsequent filtering applied to the image
- **Colorspace Considerations**: For the most accurate results, detection should ideally be performed in the same colorspace in which the image was originally resized, such as linear RGB
- **Video Analysis**: When analyzing video content, resdet is most effective with keyframes that have a low quantizer, and yuv4mpeg streams are preferred due to their superior preservation of chroma planes
- **False Positives**: A notable practical limitation is the potential for false positives when analyzing moderate to heavily compressed JPEG files, although the application of a deblocking filter can help mitigate this issue
- **Neural Network Limitations**: Crucially, this method is fundamentally designed for traditional resampling techniques and will not work with newer neural network-based resizers due to their generative nature, which synthesizes new details rather than merely interpolating existing ones

### 4.2 Error Measurement (Trial and Error) Method

The Error Measurement method for native resolution detection, while potentially slower than Fourier Transform analysis, offers distinct advantages, including its relative simplicity of implementation and its capacity to identify instances where multiple native resolutions might exist within a single image. This method operates on an iterative "trial and error" principle.

#### The Trial-and-Error Process

The process begins by employing an inverse resizer to systematically downscale the source image to a wide range of possible resolutions. This range can be comprehensive (e.g., from 400p to 1079p) or a more focused, user-defined "sane range" based on common production resolutions. For each hypothetical native resolution tested, the resulting downscaled image is then immediately upscaled back to the original source resolution using the same resize kernel. Finally, the newly upscaled image (which has undergone a downscale-upscale cycle) is rigorously compared against the original source image.

#### Core Principle: Error Minimization

The core principle underpinning this method is that when the correct native resolution is identified, the error—defined as the absolute difference between the original image and the image that has been downscaled and then re-upscaled—will be significantly smaller and more distinct than the errors observed for all other tested resolutions. This pronounced "dip" or minimum in the error plot serves as a clear indicator of the true native resolution.

An example implementation, provided for the Vapoursynth frame server, illustrates this process through a `get_error` function. This function calculates the absolute difference between the source and the upscaled image (|x - y|) and then computes the luma_error, which is the average of the plane statistics of this difference, yielding a numerical value between 0 and 1.

#### Practical Implementation

Practical tools such as getnative and GetFnative are built upon this error measurement principle. These scripts automate the iterative process, generating plots of error versus resolution and suggesting potential native resolutions based on the identified error minima.

The getnative tool, for instance, allows users to specify a frame for analysis, a resize kernel (bicubic by default), and various parameters like b and c for bicubic, or taps for Lanczos, along with aspect ratio and height ranges. The GetFnative script further extends this by supporting fractional native resolutions and handling scenarios involving cropping and letterboxing.

#### Advantages and Limitations

The advantages of the Error Measurement method include its relative simplicity of implementation and its unique capacity to identify instances where multiple native resolutions might exist within a single image, a common occurrence in synthetic media. However, it is generally slower than Fourier transform methods, though still sufficiently fast for practical application (a few seconds per frame without multithreading), as typically only a few representative frames per video need to be analyzed.

A critical prerequisite for accurate results is the careful selection of a "good frame" for analysis—one that is bright, exhibits minimal blur, contains few post-processed elements, and features clear lineart. This consistent advice across different methodologies highlights that the accuracy and reliability of native resolution detection are profoundly dependent on the "cleanliness" and fidelity of the source material. Any pre-existing noise, heavy compression artifacts (e.g., from highly compressed JPEGs), or complex post-processing (e.g., heavy visual effects, dynamic grain) will significantly degrade the effectiveness of these detection algorithms.

This leads directly to the practical advice to "TRUST YOUR EYES OVER THIS SCRIPT!" and "Trust your gut for 1080p", as algorithmic output can become unreliable when faced with highly imperfect data.

#### Native 1080p Detection Limitation

A significant functional limitation of tools like getnative is their inability to automatically recognize content that is natively 1080p. This "blind spot" is inherent to the methodology. If a source is already native 1080p, there is no upscale to reverse. When the tool attempts to downscale from 1080p (itself native) to a hypothetical lower resolution (e.g., 720p) and then upscale back to 1080p, it will always introduce some error due to the interpolation process. However, this error will not exhibit a distinct, sharp "dip" that points to a lower native resolution, because no such lower native resolution exists. Instead, the error might be uniformly low or lack a clear minimum, failing to provide the characteristic signature the algorithm looks for.

This causal relationship between the method's design (seeking a lower native resolution) and the nature of truly native high-resolution content explains why human judgment remains crucial for confirming 1080p sources.

### 4.3 Comparison of Detection Methodologies

| Method            | Pros                                                                     | Cons                                                                  | Tools/Examples                                                           |
| ----------------- | ------------------------------------------------------------------------ | --------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| Fourier Transform | Fast, visual (graphs), identifies specific signatures                    | Misses multi-res; sensitive to compression; geo-tagged rare           | Anibin (Japanese blog); resdet; kbz12.pdf figs (native vs. upscale freq) |
| Error Measurement | Accurate for multi-res; automated graphs; handles fractional resolutions | Slower; needs good frames (bright, clean); cannot detect native 1080p | getnative (graphs); GetFnative (fractional search)                       |

The complementary nature of Fourier Transform analysis and Error Measurement methods suggests that an expert workflow would likely integrate both: FT-based tools for rapid initial screening of clean sources, and EM-based tools for precise identification and handling of complex scenarios.

### 4.4 Real-World Examples and Case Studies

#### Examples from Various Sources

- **Anibin**: "K-On! Season 2" (715p eval, 1280x720 est); "Attack on Titan" (824p/857p eval)
- **encode.moe**: Manaria Friends (878p via graph); bad graphs (Miru Tights, no clear spikes)
- **silentaperture**: Nichijou (720p bilinear)
- **kageru.moe**: Non Non Biyori Repeat (846p, ringing if wrong res)

#### Studio-Specific Workflows

- **Kyoto Animation**: Historically used 955.5p for features with full HD reserved for OP/ED sequences
- **CloverWorks**: Employs Lanczos-878 upscaling with minimal taps, causing haloing and moiré that require post-processing cleanup
- **Hybrid Workflows**: Modern productions mix 2D/CG elements rendered at different resolutions, complicating detection

#### Evolution of Techniques

- **Early Sources (2010s)**: kageru.moe (legacy, pre-2017) and kbz12.pdf (2017 unfinished) focus on basics: Manual Avisynth/Vapoursynth scripting, Anibin for res lookup, simple masks. Emphasis on bilinear/bicubic; warns against upscaling with inverse kernels.
- **Mid-2010s Anibin**: Manual Fourier analysis; explains fractional res from 1088p upscale/crop (consistent across sources).
- **Modern Guides/Tools (2020s)**: encode.moe and silentaperture (post-2020) integrate tools like getnative/descale; stress visual checks, masks for credits. GitHub tools add automation, fractional support (e.g., GetFnative for odd sizes like Anibin's 955.5p).

**Consistency**: All agree on kernels (bilinear common), error/Fourier methods, masks for artifacts. Evolution: From manual (error-prone) to tools with graphs/fractionals. Warnings uniform: Verify visually; avoid on bad sources.

**Differences**: Old sources use Avisynth heavily; modern favor Vapoursynth. Tools like resdet add freq focus; getfscaler emphasizes professional kernels.

## 5. Practical Implementation of Descaling

The theoretical understanding of native resolution detection and inverse resizing translates into practical application through a suite of specialized software tools and meticulous workflows. The Vapoursynth frame server, along with dedicated plugins and Python scripts, forms the backbone of modern descaling operations.

### 5.1 The Vapoursynth Ecosystem for Descaling

Vapoursynth is a powerful, open-source frame server designed for video processing. It provides a flexible and extensible environment where various filters and plugins can be chained together to perform complex video manipulations. Its Python scripting interface allows for highly customizable and precise control over every aspect of video processing, making it an ideal platform for sophisticated tasks like descaling.

#### Core Descaling Plugin

At the core of descaling within Vapoursynth is the Descale plugin. This plugin is specifically designed to undo upscaling operations by applying the inverse mathematical models discussed previously. Descale supports a range of common resize kernels, including bicubic, bilinear, Lanczos, and spline upscales, which are frequently encountered in digitally produced anime content, often from resolutions such as 720p, 810p, 864p, or 900p.

The plugin itself typically supports specific input formats like GrayS, RGBS, and YUV444PS, while its Python wrappers extend compatibility to various YUV subsampling formats, Gray, and RGB of every bitdepth. The Descale plugin's functionality is often accessed through convenient aliases provided by fvsfunc, such as fvf.Debilinear, simplifying its integration into Vapoursynth scripts.

The plugin allows for precise control over kernel parameters, such as the b and c values for bicubic kernels, and supports custom kernels through lambda functions for advanced users. The Descale plugin also includes parameters for source cropping (src_left, src_top, src_width, src_height) and border handling, which are crucial for accurately aligning the descaling operation with the original content boundaries.

### 5.2 Key Tools and Scripts for Detection and Application

Several Python scripts complement the Descale plugin, automating the complex processes of native resolution detection and facilitating the application of descaling.

#### Detection Tools

**getnative.py**: This is a widely used Python script designed to find the native resolution(s) of upscaled video material, primarily anime. It implements the Error Measurement method, analyzing a given video frame to determine the original resolution before upscaling. The script generates a graph where the X-axis represents resolutions and the Y-axis displays the relative error. A clear spike showing a low relative error indicates the native resolution.

Users can specify a particular frame for analysis, choose a resize kernel (Mitchell-Netravali bicubic by default), and configure parameters like b and c for bicubic, taps for Lanczos, aspect ratio, and height range. The script requires Python 3.6+, matplotlib, Vapoursynth R45+, descale, and a source filter like ffms2, lsmash, or imwri.

For optimal results, getnative.py recommends selecting a high-quality, bright frame with minimal blur, few post-processed elements, and clear lineart. A significant limitation is its inability to automatically recognize 1080p native productions, requiring user judgment for such cases.

**GetFnative**: This script, a variant of getnative, focuses on finding native fractional resolutions of upscaled material, especially anime. It extends the "naive search" approach to find fractional src_height values within a specified interval and step length. GetFnative can handle scenarios where base_height and base_width are unknown, often only requiring their parity for descaling. It also supports descaling in only width or height to aid in identifying parity individually, and can acquire cropping parameters for descaling. A "quick" version, getfnativeq, checks preset values with limited options for faster analysis.

**getfscaler**: Described as an unofficial "companion" script to getfnative, getfscaler is a rewrite of the original getscaler tool, providing fractional descaling support using vsjet packages. It employs more robust JET tooling, defaults to kernels found in professional software plus Point, and includes post-filtering methods to reduce errors from dirty edges and dithering. It supports fractional descaling, cross-converted video, and more image types without relying on ffms2. The script provides warnings and information, and offers optional features like one-dimensional scaling and debug output. A crucial warning accompanying getfscaler is that its results are not conclusive on their own and may be inaccurate, emphasizing the need to "TRUST YOUR EYES OVER THIS SCRIPT!".

**resdet**: This command-line tool and C library detects upscaling and determines original resolution by analyzing the image's frequency domain using the Discrete Cosine Transform. It identifies "zero-crossings" unique to traditional resampling methods, which correspond to the original resolution. resdet works best on clear, detailed images and is sensitive to compression artifacts and filtering, performing optimally in the original resize colorspace (e.g., linear RGB). It is limited to traditional methods and will not work with neural network-based resizers.

#### Specialized Detection Tools Summary

| **Tool**                 | **Functionality**                                      | **GitHub Repo**                           |
| ------------------------ | ------------------------------------------------------ | ----------------------------------------- |
| `getnative`/`getfnative` | Estimates native resolution via edge sharpness metrics | Infiziert90/getnative, YomikoR/GetFnative |
| `resdet`                 | Statistical analysis of compression artifacts          | 0x09/resdet                               |
| `Jaded-Encoding`         | Descaling filters for reverse engineering              | Jaded-Encoding-Thaumaturgy/getfscaler     |

### 5.3 The Descaling Workflow: From Detection to Verification

The practical descaling process involves a systematic workflow, from identifying the native resolution and kernel to verifying the quality of the inverse scale.

#### Step 1: Preparation and Frame Selection

Before initiating descaling, two primary tools are needed: getnative.py for resolution detection and BluBb-mADe's descale for the actual descaling process. The first critical step is to select a "good frame" for analysis. This should be a high-quality, bright frame with minimal blur, few post-processed elements, and clear lineart. Examples of "bad" frames include dark scenes, those with heavy effects, or dynamic grain, as these can obscure the underlying scaling artifacts and lead to inaccurate detection.

If the frame contains letterboxing, it must be cropped out prior to analysis to prevent distorted graphs. This rigorous frame selection is vital because the accuracy of native resolution detection is profoundly dependent on the "cleanliness" and fidelity of the source material. Any pre-existing noise, heavy compression artifacts, or complex post-processing will significantly degrade the effectiveness of these detection algorithms.

#### Step 2: Finding the Native Resolution

The selected frame is then processed using getnative.py. The script generates a graph plotting resolution against relative error. The objective is to identify a clear, distinct spike showing a low relative error, which indicates the native resolution. For instance, a graph might clearly point to 878p as the native resolution.

It is crucial to watch for "bad" graphs that are unclear or lack a single, distinct spike; in such cases, descaling is too risky and can be destructive. As noted, getnative.py cannot find native 1080p elements, requiring encoder judgment for such sources. The error measurement method, which getnative employs, works by attempting to reverse a hypothetical upscale. If the source is already native 1080p, there's no upscale to reverse, and the process of downscaling and re-upscaling will always introduce some error, but without the characteristic "dip" that signals a lower native resolution. This explains why human judgment remains crucial for confirming 1080p sources.

#### Step 3: Determining the Upscale Kernel

Once a candidate native resolution is identified, the next critical step is to determine the specific upscale kernel used by the studio or publisher. While getnative.py defaults to Mitchell-Netravali (a bicubic variant), studios may use other kernels. The most reliable method is a comparative trial-and-error approach: testing a variety of common kernels (e.g., Lanczos, Spline16, Spline36, Bilinear, various Bicubic settings) and closely examining the results for scaling-related artifacts like haloing or ringing.

For bicubic kernels, values often follow patterns like b+2c=1, b=0,c=X, or b=1,c=0.

#### Step 4: Verification through Rescale Test

To confirm the correct kernel and resolution, a rescale test is performed. This involves descaling the source video using the identified kernel and resolution, then immediately upscaling the descaled video back to the original source dimensions using the same kernel. The chroma from the original source is typically merged with the rescaled image, as chroma often has a lower resolution and cannot be descaled effectively.

The original source and the rescaled image are then interleaved for direct comparison. The expectation is that the lineart in the rescaled image should be "practically identical" to the original, with no additional haloing or aliasing introduced. Zooming in at 4x magnification or higher is recommended for this comparison to discern subtle differences. An incorrect descale and rescale will show "lots more artifacts".

This verification step is crucial because, as noted, perfect descaling is unattainable due to the non-lossless nature of consumer sources like Blu-rays, which always introduce small, albeit often negligible, differences from the original masters.

#### Step 5: Dealing with Mixed Resolutions and Challenges

Many videos contain elements at different resolutions within the same frame, such as a background in 900p and a character in 720p, or 1080p credits and overlays within a lower-resolution video. Descaling the entire frame in such cases can introduce haloing artifacts to the elements that are already native 1080p.

To address this, fvsfunc provides DescaleM functions (e.g., fvf.DebilinearM), which mask elements at different resolutions and scale them using a standard spline36 resize. While slower, these functions can be applied selectively to frames containing such elements. Wrappers like inverse_scale from kagefunc can also automate this masking.

Examining the mask generated by these wrappers is important; if the mask catches "other stuff" beyond the intended elements, it may indicate an incorrect kernel or that the frames are truly native 1080p and should not be descaled.

In rare cases where resolution or kernel changes frame-by-frame, lvsfunc.scale.descale attempts automatic height detection, though manual intervention is often ideal. If a scene continues to exhibit issues after descaling, potential solutions include trying a different kernel/values, applying an Eedi3 filter (a slow but effective anti-aliasing filter for bad lineart), or, in some cases, deciding not to descale the clip at all. If working with a poorly descaled encode from another source, finding a different encode is often the best solution.

### 5.4 Recommended, Repeatable Workflow

Below is the workflow recommended by experts. Do not rely on a single frame or single tool — use both spectral and error-metric methods and always visually inspect results.

#### Phase 0: Preparation

- Work on a lossless extract of the video when possible (or highest quality few frame grabs)
- For video: extract a handful (3–10) representative frames that are bright and detailed and do not contain credits/overlays when possible
- Guides & tool READMEs recommend bright scenes for best detection

#### Phase 1: Fast Spectral Check (resdet)

- Build/run resdet on a set of frames (it's fast, C tool)

```bash
# example: detect single frame (PNG) — resdet autodetects best guess
resdet frame.png
# for multiple frames, pass a y4m stream or a small y4m set (see README)
```

If resdet returns a clear integer native size (e.g. best guess: 1280x720) that's a strong signal. If it returns ambiguous results or unusual fractional numbers, move to the trial method.

#### Phase 2: Trial / Error-Metric Sweep (getnative / kbz12 method)

- Use getnative (Vapoursynth helper) or the pseudocode in kbz12: for each candidate native size (e.g. heights 400p–1079p, or a focused range from resdet), do:
  1. inverse-rescale (descale) to candidate width×height
  2. rescale back to source resolution with the same kernel type used by the studio (bilinear/bicubic)
  3. compute absolute difference / plane stats between original and re-upscaled image
  4. the correct native size yields a local minimum in that error metric

This is exactly the method described in the kbz12 paper and implemented by getnative. Example pseudocode from kbz12 (conceptual):

```python
down = inverse_kernel(source, w, h)
up = upscale_bilinear(down, source.width, source.height)
error = mean_absolute_difference(source, up)  # PlaneStatsAverage
```

Run this on multiple frames and average or inspect the error graphs; consistent minima across frames are reliable.

#### Phase 3: Fractional / Odd Sizes (GetFnative / getfscaler)

- If the trial shows minima at fractional widths/heights (or weird offsets, e.g., 955.5 vertical), use GetFnative / getfscaler which are designed to search fractional sampler offsets and fractional scales used by real studio workflows (and are part of the JET ecosystem). They will attempt to locate fractional subpixel offsets, crop offsets, or 1088→1080 crop scenarios.

#### Phase 4: Descale with Masking (the Actual Inverse Operation)

- Use the descale plugin (or fmtconv with invks=True) to produce the inverse kernel result. For example in Vapoursynth (conceptual):

```py
# using fmtconv (kageru / example)
y = core.std.ShufflePlanes(src, 0, vs.GRAY)
y = core.fmtc.resample(y, 1280, 720, kernel='bilinear', invks=True)
# upsample chroma with spline, merge to YUV444, apply mask (MaskDetail/MaskedMerge)
```

If there are overlays (credits), produce a mask from the differences between source and Descale→Upscale and use MaskedMerge to keep the overlay from being inverse-scaled (kageru calls this MaskDetail approach). The kageru import script is a ready example.

#### Phase 5: Validation (Automated + Human)

- **Automated**: Recompute the mean absolute difference (PlaneStatsAverage) between the original source and [descaled→rescaled]. The error should be near the minimum you saw during discovery. Kageru/kbz12 provide code and figures for this check.
- **Human**: Quickly scrub samples (especially text/credit heavy scenes and high-detail backgrounds). Look for ringing, halos, color shifts, or broken outlines — these are red flags.

#### Phase 6: Produce Final Filtered Output

- If you plan to release in 720p: after descaling you may apply your chosen upscaler (re-upscale with a good kernel or leave to player), filter, encode
- If you want to store native resolution as archive: keep the descaled source (clean, smaller file) plus source and a short test report (frame comparisons and index of frames used)

### 5.5 Example Commands and References

#### resdet Example (from README)

```bash
magick frame.png -resize 150% resized.png
resdet resized.png
# output example: "best guess: 512x512"
```

(Resdet prefers y4m for video frames and has tips for deblocking JPEGs.)

#### Descale Plugin Usage

descale.Debilinear(clip, width, height) and descale.Debicubic(clip, width, height, b, c) (see plugin wrapper docs). Use the plugin wrapper/py for convenience; the plugin expects certain planar formats (see README).

#### getnative / getfnative

Read their READMEs and run them on a few frames; they are Vapoursynth-centric tools and expect a Python/Vapoursynth environment. They explicitly warn that bright/detailed frames give better results and that you should test multiple frames.

### 5.6 Short Checklist

1. Pick 5–10 frames (avoid credits)
2. Run resdet on frames. If resdet yields a clear integer candidate, try that first
3. Run getnative (error sweep) across a window around the candidate(s). Average/compare frames
4. If fractional/odd: run GetFnative / getfscaler
5. Descale with descale / fmtconv + invks=True, split planes and handle chroma as recommended. Mask credits
6. Validate with PlaneStatsAverage / visual check. Use the kbz12 error measure to confirm

### 5.7 Real-World Encoding Practices

#### Beatrice-Raws Case Study

- Cleaned Lanczos upscaling artifacts using frequency filtering and detail-preservation masks
- Reduced DTS-HD MA audio bit depth to 20-bit to accommodate video bandwidth constraints

#### Descaling Workflows

- Reverse upscaling using Bicubic/Bilinear kernels to approximate source resolution
- Tools like getfscaler automate kernel selection for optimal descaling

### 5.8 Scripts and Examples

#### kageru.moe Importable Vapoursynth Script

```py
# Importable Vapoursynth for bilinearM (masks, 4:4:4)
out = deb.debilinearM(src, 1280, 720, kernel='bicubic')
```

#### encode.moe Examples

- Comparison kernels on Manaria Friends frame
- Mask for Kaguya-sama credits

#### silentaperture Examples

- Nichijou descale vs. spline36

### 5.9 What Each Resource Contributes

- **kageru.moe (legacy article + script)** — practical, hands-on recipes: debilinear/debicubic, masking for credits, splitting luma/chroma and producing 4:4:4 outputs; includes an importable Vapoursynth script debilinearM and an explanation of masks and subsampling. Useful as a working example.

- **kbz12.pdf (Kageru paper)** — rigorous explanation: kernel math, why inversion is possible (A·x=b viewpoint), Fourier method for identifying missing high frequencies, and the error-measurement / trial approach (descale → re-upscale → compare). Good theory + short pseudocode for an error function.

- **guide.encode.moe** — up-to-date fansubbing guide that covers practical pitfalls and stepwise filtering pipelines (descale → filter → reupscale) and emphasizes testing/visual checks. Good "how you should do it in a release".

- **silentaperture / mdbook guide** — another practical writeup that emphasizes descaling benefits and cautions (bad descaling gives halos, ringing). Useful second opinion on recommended tooling and masks.

- **anibin (archived)** — the classic FFT-based investigations on native / fractional, real examples of odd fractional sizes and studio quirks — extremely valuable as a reference dataset and for intuition.

- **resdet** — a C utility (lib + CLI) implementing spectral detection (FFT) to guess the source resolution from upscaled images/frames. Fast and robust on clean frames; tends to fail on neural resizers and heavy compression. Use it as the first, fast check.

- **getnative** — a Vapoursynth/Python tool using the error measurement (trial descales and computes plane stats) to find candidate native sizes; recommended to run on several frames because success varies by scene brightness/detail.

- **GetFnative / getfscaler** — scripts to find fractional native resolutions (non-integer scaling factors) and helper wrappers for fractional descaling (JET family). Useful for those awkward non-integer studio cases and for the modern JET toolchain.

- **descale plugin** — the canonical Vapoursynth/AviSynth plugin used to actually undo linear interpolation (Debilinear / Debicubic etc.). Use this for the core inverse operation; wrapper scripts will handle chroma planes and convenience.

## 6. Challenges, Limitations, and Best Practices

### 6.1 Common Failure Modes and How to Spot Them

Despite the sophisticated tools and methodologies available, descaling operations can fail for various reasons. Understanding these failure modes is crucial for successful implementation.

#### 1. Credits / Native 1080p Overlays

**Problem**: Ringing/artifacts around text or graphics that are already at native 1080p resolution.

**Detection**: Look for haloing or ringing artifacts specifically around text, credits, or UI elements.

**Solution**: Mask those areas and fall back to a conventional resizer for masked areas. Kageru shows masked merges using MaskDetail approach.

**Implementation**: Use DescaleM functions (e.g., fvf.DebilinearM) or inverse_scale from kagefunc with mask_detail=True.

#### 2. Studio Post-Processing Applied After Upscale

**Problem**: Sharpening, grain, or noise injection applied after the upscale operation makes perfect inversion impossible.

**Detection**: Masked detail map shows residuals across the image; descaling produces halos or missing detail.

**Solution**: Cannot perfectly invert; descaling will produce halos or missing detail. May need to skip descaling or apply post-processing cleanup.

**Examples**: A-1 Pictures often applies post-production sharpening that mimics upscaling artifacts.

#### 3. Chroma Subsampling Mismatch

**Problem**: If you only descale luma, chroma may be undersampled, leading to color bleeding.

**Detection**: Color artifacts, particularly around high-contrast edges.

**Solution**: The recommended approach is to descale luma and properly resample chroma, or produce 4:4:4 outputs to avoid color bleeding. Kageru covers plane splitting for this.

**Implementation**: Split planes, descale luma, upsample chroma with spline, merge to YUV444.

#### 4. Neural Network Upscalers

**Problem**: Not reversible by deterministic algorithms and will confuse spectral tools.

**Detection**: resdet explicitly warns about NN upscalers; tools fail to find clear resolution signatures.

**Solution**: Cannot be reversed with current deterministic methods. May need to skip descaling entirely.

**Examples**: waifu2x, Google's RAISR, and other generative upscalers.

#### 5. Heavy Compression / JPEG Artifacts

**Problem**: Spectral detectors can produce false spikes (multiples of 1/8).

**Detection**: resdet README documents this issue; inconsistent results across frames.

**Solution**: Apply deblocking before analysis; use higher quality sources when possible.

**Mitigation**: resdet has built-in deblocking options for JPEG analysis.

### 6.2 False Positives and Detection Challenges

#### Post-Production Sharpening

Post-production sharpening (e.g., A-1 Pictures) can mimic upscaling artifacts, leading to false positives in detection algorithms.

#### Cel-Shaded Animation

Cel-shaded animation lacks high-frequency details, which can confuse spectral tools that rely on frequency analysis.

#### Mixed-Content Sources

Modern productions mix 2D/CG elements rendered at different resolutions, complicating detection and requiring sophisticated masking approaches.

### 6.3 Tooling Gaps and Limitations

#### Machine Translation Limitations

Machine translation (MTL) fails for manga due to cultural nuances/slang, limiting automated analysis of Japanese content.

#### Framework Limitations

GPU buffer access limitations in frameworks like Flutter can hinder efficient screen capture for analysis.

#### Community Verification Issues

Crowdsourced databases (e.g., AniBin) require skepticism due to unverified methodologies and potential inaccuracies.

### 6.4 Best Practices for Optimal Native Resolution Restoration

#### 1. Prioritize Source Quality

Always begin with the cleanest possible source material. Minimize pre-existing compression artifacts, noise, and heavy post-processing effects, as these significantly degrade the accuracy of native resolution detection and the quality of the descaling process. If multiple encodes are available, select the one with the highest fidelity.

#### 2. Employ Complementary Detection Methodologies

Do not rely on a single detection tool or method. Utilize both Fourier Transform-based tools (like resdet for quick initial assessments on clean sources) and Error Measurement-based tools (like getnative or GetFnative for precise, iterative analysis and fractional resolution detection). Cross-reference results to build confidence in the identified native resolution.

#### 3. Meticulous Frame Selection

When using Error Measurement tools, carefully select "good frames" for analysis. These frames should be bright, have clear lineart, and be free from excessive blur, dynamic grain, or complex visual effects. Crop out any letterboxing before analysis to avoid skewed results.

#### 4. Iterative Kernel Identification

The precise identification of the original upscale kernel is paramount. This often requires a systematic, comparative trial-and-error approach, testing various common kernels (Bilinear, Bicubic with different b/c values, Lanczos, Spline) and visually inspecting the results for the minimization of artifacts like haloing and ringing. Leverage Vapoursynth scripts that allow for quick descale-rescale comparisons.

#### 5. Rigorous Verification

Always perform a rescale test to verify the accuracy of the descaling. Descale the content with the identified parameters, then upscale it back to the original dimensions using the same kernel. Interleave the original and the processed frames for pixel-perfect comparison, ensuring lineart is virtually identical and no new artifacts are introduced.

#### 6. Strategic Handling of Mixed Resolutions

Be prepared for content with mixed native resolutions (e.g., 1080p credits on a 720p animated background). Utilize masking functions (e.g., DescaleM in fvsfunc or inverse_scale in kagefunc) to apply descaling only to the relevant lower-resolution elements, while preserving higher-resolution overlays or graphics.

#### 7. Acknowledge Limitations

Understand that deterministic descaling cannot reverse upscales performed by generative neural networks (e.g., waifu2x, RAISR) due to their synthetic detail creation. For truly native 1080p content, rely on informed judgment, as error measurement tools may not provide a distinct "dip."

#### 8. Prioritize User Experience

Consider the end-user's playback environment. Releasing content in its native resolution allows advanced video players (e.g., MadVR) to perform high-quality, real-time upscaling, potentially delivering a superior viewing experience than a pre-upscaled release.

### 6.5 Practical Tips and Warnings

#### Best Practices Summary

- Use Blu-ray sources when possible
- Test multiple frames and kernels
- Mask credits and overlays appropriately
- For bad descaling results: apply EEDI3 anti-aliasing or skip descaling entirely
- Trust visual inspection over algorithmic output when results are unclear

#### Common Pitfalls

- Dark or grainy frames give bad detection graphs
- Native 1080p content is undetectable by automated tools (rely on human judgment)
- Fractional resolutions are common (especially from Anibin examples)
- Don't release odd resolutions like 873p (leechers may ignore such releases)

#### Resource Recommendations

- Anibin for lookups (Kanji titles via MAL/AniDB)
- Vapoursynth Database for plugins and community support
- Studio-specific knowledge helps with kernel selection

#### Quality Claims

Descaling can improve quality but isn't magic—substantiated by examples showing reduced artifacts and file size. The improvement is most noticeable when reversing "cheap" upscales and re-applying higher-quality algorithms.

## 7. Future Directions and Emerging Technologies

### 7.1 Hybrid Approaches

Combining resdet's statistical models with getfnative's edge analysis could provide more robust detection capabilities, especially for complex sources with mixed content.

### 7.2 AI-Assisted Workflows

Training CNNs on studio-specific upscaling fingerprints could enable more accurate detection and potentially even reversal of some neural network upscalers.

### 7.3 Community Verification Systems

Improved crowdsourced databases with verification mechanisms could provide more reliable reference data for native resolution detection.

### 7.4 Adaptive Filtering Pipelines

Emerging solutions focus on open-source collaboration and adaptive filtering pipelines that can handle increasingly complex production workflows.

### 7.5 Explainable AI for Upscaling

Future, more "explainable" or structured AI upscaling architectures might possess some degree of reversibility, contrasting with the general difficulty of inverting more complex, black-box generative models.

## 8. Conclusions and Recommendations

### 8.1 Summary of Key Findings

The comprehensive analysis of native resolution restoration reveals it to be a sophisticated and indispensable technique within digital media processing, driven by both economic imperatives and the pursuit of visual fidelity. The practice of descaling, which involves reversing upscaling operations, addresses a pervasive industry challenge where content is often upscaled suboptimally to meet contemporary resolution standards for distribution.

The primary motivations for descaling are clear: significant reductions in file size and bandwidth consumption, leading to substantial cost savings for distributors and improved efficiency for archivists. Furthermore, descaling optimizes computational workflows by allowing subsequent processing and compression to occur on smaller, native-resolution data. Critically, it enables a two-stage quality enhancement process: undoing a "cheap" initial upscale to provide a clean, native-resolution base, which can then be re-upscaled using superior algorithms to achieve a subjectively sharper output with fewer artifacts.

### 8.2 Theoretical Foundation

The theoretical foundation of descaling rests on the deterministic nature of traditional resizing kernels, allowing their inversion through linear algebraic methods such as LDL^T decomposition. This mathematical elegance, however, encounters practical limitations due to real-world data imperfections like compression artifacts and noise, rendering perfect descaling "near-lossless" rather than truly lossless. This distinction is crucial for managing expectations and guiding practical application.

### 8.3 Detection Methodologies

Two principal methodologies, Fourier Transform analysis and Error Measurement, offer complementary approaches to native resolution detection. Fourier Transform methods, exemplified by resdet, are fast and effective for clean sources, identifying unique frequency signatures of upscaling. Error Measurement methods, implemented in tools like getnative and GetFnative, are more robust for precise resolution identification, including fractional values, and can detect multiple native resolutions within a single frame, albeit at a slower pace.

The consistent emphasis across both methodologies on the "cleanliness" of the source material highlights a fundamental truth: the accuracy of detection is highly sensitive to pre-existing noise, compression artifacts, and complex post-processing. This underscores why human judgment remains indispensable, particularly for confirming truly native high-resolution content (e.g., 1080p), which algorithmic tools may struggle to identify due to their inherent design.

### 8.4 Software Ecosystem

The practical application of descaling relies on a mature and sophisticated software ecosystem. The Vapoursynth frame server provides the foundational platform, with specialized plugins like Descale performing the actual inverse resizing operations. Complementary tools like resdet, getnative, GetFnative, and getfscaler automate detection and facilitate application, while community resources provide practical examples and workflows.

### 8.5 Challenges and Limitations

Despite its considerable benefits, native resolution restoration presents inherent complexities and limitations. Challenges include accurately identifying the original resize kernel and its parameters, dealing with content that exhibits mixed resolutions, and mitigating the detrimental impact of compression artifacts and pre-existing noise on detection accuracy. A significant current limitation is the inability of deterministic descaling methods to reverse upscales generated by advanced neural network-based algorithms, which synthesize new details rather than merely interpolating existing ones.

### 8.6 Final Recommendations

By adhering to the principles outlined in this guide and leveraging the sophisticated tools available, media professionals can effectively restore the native resolution of digital content, leading to optimized storage, more efficient processing, and ultimately, a higher-quality visual experience for consumers and a more faithful preservation of digital assets.

The key to successful descaling lies in understanding that it is both an art and a science—requiring technical knowledge, practical experience, and careful judgment. While automated tools provide valuable assistance, human expertise remains essential for interpreting results, handling edge cases, and making final quality decisions.

### 8.7 Where the Research Adds Value

The research and documentation covered in this guide provide several key benefits beyond simply "using a plugin":

- **Theoretical Understanding**: The mathematical foundations (kbz12 paper) give the rigorous justification for why linear systems can be inverted and why the trial/error approach works—this helps tune parameters and understand edge cases.

- **Practical Implementation**: The kageru article and scripts provide a practical cookbook that handles real-world messy sources (masks, chroma handling). These scripts serve as a pragmatic baseline for actual implementation.

- **Fast Detection**: resdet provides a fast spectral inspection that frequently produces a reliable integer result in one command, enabling quick initial assessments.

- **Community Knowledge**: The evolution of techniques from manual scripting to automated tools demonstrates the community's ongoing refinement of methods and best practices.

### 8.8 Final Checklist for Implementation

1. **Source Assessment**: Evaluate source quality and determine if descaling is appropriate
2. **Frame Selection**: Choose 5-10 representative frames (avoid credits/overlays)
3. **Detection**: Run resdet for quick spectral analysis, then getnative for detailed error measurement
4. **Kernel Identification**: Test multiple kernels and parameters to find the best match
5. **Verification**: Perform rescale test and visual inspection
6. **Masking**: Apply appropriate masking for mixed-resolution content
7. **Validation**: Check results with both automated metrics and human visual assessment
8. **Documentation**: Record parameters and results for future reference

This comprehensive approach ensures that native resolution restoration is performed effectively, safely, and with full understanding of both its capabilities and limitations.

---

## Works Cited

1. accessed January 1, 1970,
2. Descaling and Rescaling - Advanced Encoding Guide - GitLab, accessed August 11, 2025, https://silentaperture.gitlab.io/mdbook-guide/filtering/descaling.html
3. kageru.moe, accessed August 11, 2025, https://blog.kageru.moe/legacy/resolutions.html
4. 0x09/resdet: Detect source resolution of upscaled images - GitHub, accessed August 11, 2025, https://github.com/0x09/resdet
5. Deb Multimedia Packages::vapoursynth-descale, accessed August 11, 2025, https://deb-multimedia.org/dists/bookworm/main/binary-arm64/package/vapoursynth-descale
6. Welcome to VapourSynth's documentation!, accessed August 11, 2025, https://www.vapoursynth.com/doc/
7. DJATOMs-archive/vapoursynth-descale - GitHub, accessed August 11, 2025, https://github.com/DJATOMs-archive/vapoursynth-descale
8. Irrational-Encoding-Wizardry/descale: VapourSynth plugin to undo upscaling - GitHub, accessed August 11, 2025, https://github.com/Irrational-Encoding-Wizardry/descale
9. Infiziert90/getnative: Find the native resolution(s) of ... - GitHub, accessed August 11, 2025, https://github.com/Infiziert90/getnative
10. Jaded-Encoding-Thaumaturgy/getfscaler: getscaler with ... - GitHub, accessed August 11, 2025, https://github.com/Jaded-Encoding-Thaumaturgy/getfscaler
11. YomikoR/GetFnative: A script that help find the native ... - GitHub, accessed August 11, 2025, https://github.com/YomikoR/GetFnative
12. Descaling - Fansubbing Guide, accessed August 11, 2025, https://guide.encode.moe/encoding/descaling.html
13. Silent Aperture, accessed August 11, 2025, https://silentaperture.gitlab.io/mdbook-guide/filtering/descaling.html
14. DCT Python Implementation, accessed August 11, 2025, https://github.com/joaocarvalhoopen/Detecting_the_original_resolution_of_an_upscale_image_DCT
