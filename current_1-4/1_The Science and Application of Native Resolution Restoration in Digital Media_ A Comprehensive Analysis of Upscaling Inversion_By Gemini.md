The Science and Application of Native Resolution Restoration in Digital Media: A Comprehensive Analysis of Upscaling Inversion




1. Executive Summary


The digital media landscape is increasingly populated by content that has undergone various forms of upscaling, particularly older productions re-released to conform to contemporary hardware standards, such such as Blu-ray re-releases of standard definition anime or UHD Blu-ray versions of previously lower-resolution animation and live-action movies.1 This widespread practice has given rise to the specialized field of native resolution restoration, commonly known as descaling, which involves the precise reversal of these upscaling operations. This report provides a comprehensive analysis of descaling, encompassing its theoretical underpinnings, practical methodologies, and the significant implications for content quality, storage, and distribution.
The primary advantages derived from accurately restoring native resolutions are multifaceted. Firstly, it leads to substantial reductions in file size and associated bandwidth consumption during distribution, as an upscaled 1080p video from an original 720p source, despite its larger size, contains no additional information.1 This efficiency extends to long-term archival storage, where conserving hard drive space is a critical concern.1 Secondly, processing and compression tasks become significantly more efficient at native resolutions, as the lower pixel count per frame reduces computational burdens and processing time for video filters and compression techniques.1 Lastly, and crucially for visual fidelity, descaling enables the subsequent re-upscaling of content using more sophisticated and higher-quality algorithms, which can yield a subjectively superior high-resolution image with fewer artifacts like haloing or ringing, compared to simply applying standard resizing methods directly to an already upscaled source.1 This two-stage process, where a "cheap" initial upscale is undone to allow for a superior re-upscale, often results in a marked improvement in perceived visual quality for the end-user.1
Two principal methodologies dominate the detection of native resolutions: Fourier Transform analysis and the Error Measurement method. Fourier Transform analysis, often implemented by tools like resdet, identifies unique frequency domain signatures, such as "zero-crossings," that are characteristic of traditional upscaling operations and directly correspond to the original resolution.1 The Error Measurement method, exemplified by tools such as
getnative and GetFnative, employs an iterative trial-and-error approach where the image is downscaled to various candidate resolutions, then re-upscaled, and the absolute difference (error) from the original is calculated. A significant dip in this error indicates the true native resolution.1
The practical application of descaling relies heavily on a robust software ecosystem. The Vapoursynth frame server serves as a foundational platform, supporting specialized plugins like Descale which perform the actual inverse resizing.5 Complementary Python scripts, including
getnative, resdet, getfscaler, and GetFnative, automate the detection process, analyze image characteristics, and facilitate the application of descaling operations.4
Despite its considerable benefits, native resolution restoration presents inherent complexities and limitations. Challenges include accurately identifying the original resize kernel and its parameters, dealing with content that exhibits mixed resolutions (e.g., native elements alongside upscaled graphics within the same frame), and mitigating the detrimental impact of compression artifacts and pre-existing noise on detection accuracy.1 A significant current limitation is the inability of deterministic descaling methods to reverse upscales generated by advanced neural network-based algorithms, which synthesize new details rather than merely interpolating existing ones.1
In conclusion, effective native resolution restoration represents a powerful and essential technique for preserving content fidelity, optimizing digital media workflows, and enhancing the viewing experience for a wide range of digital content. While challenges persist, the methodologies and tools available enable media professionals to reclaim the intrinsic quality of upscaled material, leading to more efficient distribution, storage, and a superior visual presentation.


2. Introduction to Digital Image Resizing and Native Resolution




2.1. Defining Image Resizing and Scaling: Upscaling vs. Downscaling


Image resizing, often interchangeably referred to as scaling, constitutes a fundamental operation within digital image processing. Its core function is to systematically alter the dimensions of a given image or video frame without fundamentally distorting its overall structure or the integrity of its content.1 This process is achieved through sophisticated algorithms that either interpolate new pixel data when increasing size or sample existing data when decreasing size.
The operation of increasing an image's dimensions is termed upscaling, or magnification. During upscaling, the computational task involves generating new pixels to fill the expanded grid. These new pixel values are not derived from novel information but are instead estimated or interpolated based on the values of the existing, original pixels.1 A critical implication of this interpolation-based approach is that upscaling does not genuinely introduce new information or fine detail that was not present in the original source. Instead, it effectively spreads the existing information across a larger number of pixels. This often results in a perceived softening of the image or, if performed suboptimally, the introduction of visual artifacts such as aliasing or ringing. The understanding that upscaling is primarily an interpolative, rather than generative, process is foundational to the feasibility and efficacy of descaling. If upscaling were to genuinely create unique, uninferable information, the inverse process of descaling would be rendered impossible.
Conversely, downscaling, or minification, involves reducing the resolution or dimensions of an image. This process typically necessitates the selective discarding of pixel information and the averaging of existing data to fit the smaller grid. While downscaling can be optimized to preserve detail, it inherently involves a loss of some fine information due as pixels are consolidated.
The fundamental distinction between upscaling and downscaling, particularly the interpolative nature of the former, is paramount for comprehending the rationale behind descaling. Descaling leverages the deterministic nature of many upscaling algorithms to reverse the interpolation, thereby recovering the original pixel values as accurately as possible.


2.2. The Concept of "Native Resolution" in Digital Media Production


The term "native resolution" holds a precise and critical meaning within the domain of digital media. It is defined as the original resolution at which any given material was initially produced.1 This encompasses the diverse array of content creation methodologies, whether the material was originally captured through filming, scanned from physical media, meticulously drawn, animated frame-by-frame, or generated through other digital processes. The native resolution, therefore, represents the true, intrinsic resolution of the source data before any subsequent scaling operations are applied.
To illustrate this concept, consider a common scenario in video distribution: a video file that was originally filmed or animated in 720p (1280x720 pixels) but was subsequently upscaled to 1080p (1920x1080 pixels) for its Blu-ray release. In this specific instance, while the final distributed file possesses a resolution of 1080p, its native resolution unequivocally remains 720p.1 This distinction is not merely semantic; it is crucial for understanding the core purpose and technical objectives of descaling. The goal of descaling is to revert the content to this original, pre-upscale state.
A nuanced aspect of native resolution arises in the context of synthetic media, such as computer-generated imagery (CGI). In such productions, it is technically possible for a single frame to comprise elements that were produced at multiple distinct native resolutions.1 For example, a background might be rendered at a higher resolution, while foreground characters or specific visual effects might be generated at a lower resolution and then composited and upscaled to match the overall frame. This inherent complexity in synthetic media directly foreshadows the challenges associated with descaling content that contains mixed resolutions, a topic that warrants detailed discussion in later sections of this report. The ability to identify and address these varying native resolutions within a single frame is a hallmark of advanced descaling techniques.


2.3. The Imperative for Native Resolution Restoration: Benefits and Use Cases


The restoration of an image or video to its original native resolution, a process known as descaling, offers a compelling array of benefits that extend across various stages of the digital media lifecycle, from production and distribution to archival and end-user consumption. These advantages underscore the growing importance of descaling as a sophisticated and economically viable technique.
A primary driver for descaling is the significant reduction in file size and the consequent improvement in bandwidth efficiency. An upscaled video file, such as a 1080p version derived from an original 720p source, is inherently and substantially larger than its native-resolution counterpart.1 This increase in size occurs despite the critical fact that the upscaled file contains no genuinely new or additional information beyond what was present in the original native resolution.1 This fundamental characteristic means that the extra data in an upscaled file represents redundant or interpolated information. By restoring content to its native resolution, file sizes are directly and often dramatically decreased.3 This translates into considerable advantages for efficient digital distribution, leading to reduced bandwidth costs for streaming services and faster download times for end-users. Furthermore, for long-term archival storage, conserving expensive hard drive space is a paramount concern, and descaling offers a direct solution to optimize storage infrastructure.1 The recurring emphasis on "wasted bandwidth" and "hard drive space" in discussions of descaling underscores its role not merely as a technical refinement but as a substantial economic and resource optimization strategy. For large-scale content distributors or archival institutions, even marginal percentage savings per file, when scaled across vast libraries, translate into immense cost reductions in storage infrastructure, data transfer fees, and computational power.
Beyond storage and bandwidth, descaling critically optimizes processing and compression workflows. The higher pixel count per frame in upscaled media directly translates to increased computational demands. Tasks such as applying complex video filters, performing precise color corrections, or executing various compression algorithms become significantly more time-consuming and resource-intensive when operating on upscaled content.1 By reverting to the native resolution, these computational burdens are substantially reduced, leading to faster and more efficient encoding, post-production, and transcoding workflows.1
Perhaps one of the most compelling benefits, particularly for visual quality, is the ability to achieve enhanced perceived image quality through strategic re-upscaling. Publishers and digital media distributors frequently employ "cheap" or suboptimal upscaling techniques to adapt older content or lower-resolution productions to current hardware standards, such as 1080p or 4K Blu-ray releases.1 This practice, while meeting market demands, often introduces undesirable artifacts like haloing or ringing. A key advantage of descaling is its capacity to reverse these initial, often inferior, upscales.1 Once the content is restored to its pristine native resolution, it can then be re-upscaled using a more sophisticated and higher-quality algorithm. This two-step process can result in a subjectively superior high-resolution image, often appearing sharper with fewer artifacts compared to simply applying standard resizing methods like Spline36 directly to an already upscaled source.1 This indicates that descaling is frequently not an end in itself, but a strategic intermediate step. The primary objective often extends beyond merely returning to the native resolution; it is a tactical maneuver to "cleanse" the image of artifacts introduced by a poor initial upscale, thereby providing a pristine native-resolution base. This clean base can then be re-upscaled using a superior algorithm, either by the encoder or the end-user's playback system, which can achieve a higher perceived quality than the original upscaled release.3 This implies a powerful two-stage optimization: first, undo the damage; second, apply a better transformation.
Finally, for preservationists and archivists, restoring the original native state of content is paramount for maintaining archival integrity and fidelity to the original source. It ensures that digital media is stored and processed as close as possible to its true origin, preserving its intrinsic quality, especially in environments where strict adherence to current hardware standards is not the primary concern.1 The very existence and evolution of descaling tools and techniques within enthusiast and archival communities is a direct, community-driven response to this industry-level quality compromise, aiming to rectify the visual artifacts and inefficiencies introduced by these "cheap" upscales. This highlights a dynamic tension between commercial expediency and the pursuit of content perfection.


3. Theoretical Foundations of Image Resizing Kernels and Algorithms




3.1. Principles of Traditional Resizers


The fundamental principle underpinning all traditional, non-neural network-based image resizers is the calculation of new pixel values as a weighted average of a selected number of surrounding "reference pixels".1 The specific count of these reference pixels, denoted as 'n', varies depending on the particular kernel employed, directly influencing the complexity and visual characteristics of the resizing operation.
A range of common kernels is utilized in traditional image resizing, each with distinct properties and applications:
* Point or Nearest Neighbor: This is the simplest and fastest resizing method, characterized by n=1 reference pixel. The value of a new pixel is directly assigned from the single closest existing pixel in the source image.1 While computationally efficient, this method often results in blocky, jagged, or aliased artifacts, particularly noticeable during upscaling, as it lacks any form of interpolation to smooth transitions.
* Linear: Utilizing n=2 reference pixels, the Linear kernel performs a basic linear interpolation between adjacent pixels.1 This method offers a slight improvement over Nearest Neighbor by introducing some smoothing, but it still produces relatively soft or blurry results, especially when scaling significantly.
* Bilinear: This kernel involves n=4 reference pixels. It performs linear interpolation sequentially, first in the horizontal (x) direction and then in the vertical (y) direction.1 Bilinear interpolation produces a smoother image than Nearest Neighbor, effectively reducing jagged edges, but the resulting image can still appear somewhat soft due to the averaging nature of the interpolation. It is a common choice for quick, general-purpose scaling.
* Cubic: Also employing n=4 reference pixels, the Cubic kernel distinguishes itself by using cubic curves, such as Lagrange polynomials, for interpolation.1 This allows for a more sophisticated estimation of pixel values, often resulting in sharper outputs than Bilinear interpolation while maintaining a good degree of smoothness.
* Bicubic: Representing a more advanced form of interpolation, the Bicubic kernel utilizes n=16 reference pixels. It applies four separate cubic curves for interpolation, providing a high degree of control over the balance between sharpness and smoothness.1 Bicubic kernels are often parameterized by 'b' and 'c' values, which define the shape of the cubic function and thus influence the specific characteristics of the output, such as sharpness or the presence of ringing artifacts.2 Common variants include Mitchell-Netravali (often
b=1/3, c=1/3), Catmull-Rom (b=0, c=0.5), and Spline36.
* Lanczos and Spline: These are other widely used high-quality interpolation kernels, particularly favored for their ability to produce sharp results while effectively minimizing aliasing artifacts.2 Lanczos, for instance, uses a sinc function and is parameterized by 'taps', which determines the number of samples used for interpolation. Spline kernels, such as Spline16 and Spline36, are known for their smooth transitions and good detail preservation.
The mathematical properties of these kernels dictate their impact on image quality. A crucial characteristic is their deterministic nature: given the same input image and parameters, these traditional resizers will consistently produce the identical output.1 This predictability is the cornerstone of their theoretical reversibility, provided that no information was irrecoverably lost during the original upscaling process. Different kernels inherently introduce distinct visual characteristics, such as varying levels of sharpness, the presence of ringing (undesirable oscillations or halos around sharp edges), or haloing (light or dark borders around high-contrast areas) during upscaling. The objective of descaling is precisely to identify these specific artifactual signatures and reverse them, thereby recovering the original, cleaner image data.
The following table summarizes the common resizers and their properties as discussed:


Kernel
	n (Reference Pixels)
	Comments
	Linear
	2
	Simple linear interpolation 1
	Cubic
	4
	Uses curves (e.g., Lagrange) 1
	Bilinear
	4
	Linear in z and y direction 1
	Bicubic
	16
	Uses 4 separate cubic curves, parameterized by b/c values 1
	Point/Nearest Neighbor
	1
	Simplest, fastest; often results in blocky output 1
	Lanczos
	Varies (e.g., taps=3)
	Known for sharpness and anti-aliasing; uses sinc function 2
	Spline (e.g., Spline16, Spline36)
	Varies
	Smooth transitions, good detail preservation; often used for anime upscales, can introduce ringing/haloing 2
	This table serves as a foundational reference, consolidating crucial information about various deterministic kernels. Understanding the characteristics of these kernels is indispensable for the practical step of accurately identifying the original upscale kernel during the descaling process, as each kernel leaves a distinct signature that can be analyzed and reversed.


3.2. Deterministic vs. Neural Network Resizers


The landscape of image resizing algorithms is broadly divided into two major categories: deterministic resizers and neural network-based resizers. These two paradigms operate on fundamentally different principles, leading to significant implications for their predictability, output characteristics, and, crucially, their reversibility.
Characteristics and Predictability of Deterministic Algorithms:
As previously detailed, deterministic resizers, such as Linear, Bilinear, Cubic, and Bicubic, are predicated on fixed, explicit mathematical formulas.1 This reliance on predefined mathematical operations ensures their predictable and consistent behavior: given the exact same input image and parameters, these algorithms will invariably produce the identical output.1 This inherent determinism is the cornerstone of their theoretical reversibility. The process of inverting such a resize operation can be precisely modeled as solving a linear equation,
A∗x=b, where 'A' represents the known resize kernel (encapsulating its mathematical properties and parameters), 'x' is the unknown vector of original pixel values that one aims to recover, and 'b' is the known vector of upscaled pixel values (the input image data).1 This mathematical elegance and robustness, however, encounter practical constraints in real-world scenarios. The critical caveat "as long as no information was lost" 1 is paramount. In practice, factors such as lossy compression (e.g., from video codecs or JPEG files), subsequent filtering, and the introduction of noise inevitably introduce irreversible alterations to the pixel data. Therefore, while the mathematical framework for inversion is theoretically pure, its application in practice is constrained by the imperfections of real-world source material. This is why descaling is consistently described as "near-lossless" 2 rather than perfectly lossless. The theoretical purity of deterministic inversion is always compromised by practical data degradation, transforming the problem from a simple inverse calculation into a sophisticated optimization challenge that seeks the
most probable original state rather than the exact one.
Overview of Neural Network-Based Upscaling:
In contrast, neural network-based upscaling represents a more recent and rapidly evolving frontier in image processing. Prominent examples include waifu2x, NNEDI3, and Google's RAISR.1 These methods typically involve an initial resize, often performed using a traditional resizer, to bring the image to a target resolution. Following this, a series of multiple convolutional layers or similar filters are applied to each pixel.1 The overarching objective of these neural networks is to achieve "super-resolution" with exceptionally high sharpness and fine detail, often by "hallucinating" or synthesizing new details based on complex patterns learned from vast training datasets.1 This approach fundamentally differs from traditional interpolation. Traditional methods are fundamentally about
interpolation – estimating values between known data points. Neural networks, however, engage in hallucination or synthesis – generating new, plausible details based on learned patterns, effectively creating "information" that was not explicitly present in the original source. This generative aspect makes their inversion fundamentally different and generally intractable with current deterministic methods, as there is no direct mathematical inverse for "imagined" data. This explains why descaling techniques are primarily applicable to content upscaled using traditional, deterministic algorithms.
Challenges of Inverting Neural Network Upscales:
The generative and non-linear nature of most neural network upscalers poses significant challenges to their inversion. Unlike deterministic resizers, these algorithms produce "seemingly arbitrary outputs" that are highly dependent on the specific data used to train their underlying models.1 Their ability to synthesize details not present in the original source, effectively "imagining" new information, makes them generally non-deterministically reversible. Tools like
resdet explicitly acknowledge this limitation, stating their inability to work with "newer neural network-based resizers" due to this fundamental difference in operation.4 The process of "de-hallucinating" synthesized information, or finding a unique inverse for a generative process, is currently intractable with the inverse mathematical models used for deterministic upscales.
Special Cases: NNEDI3 Inversion:
Within the realm of neural network-based upscalers, NNEDI3 stands out as a unique exception with a degree of reversibility. It operates by interpolating only every other pixel in an image, crucially leaving the original reference pixels untouched.1 Due to this highly specific and structured interpolation pattern, its upsampling effect can be reversed relatively simply. This is achieved by removing every other row of pixels in the image, particularly effective for magnifications that are powers of 2 (i.e.,
2n for n∈N).1 This makes NNEDI3 a rare instance of a partially reversible neural network upscaler, providing a conceptual bridge between purely deterministic and fully generative AI models. Its specific reversibility suggests that not all "neural network" upscalers are designed with the same level of generative complexity. NNEDI3's interpolation method is highly structured and predictable, making it amenable to a direct inverse operation. This implies that future, more "explainable" or structured AI upscaling architectures might also possess some degree of reversibility, contrasting with the general difficulty of inverting more complex, black-box generative models.


4. Methodologies for Native Resolution Detection


Accurately identifying the native resolution of an upscaled image or video is the crucial first step in the descaling process. Two primary methodologies are employed for this purpose: Fourier Transform analysis and the Error Measurement method. Each offers distinct advantages and is suited to different scenarios and data characteristics. The effective application of these methods often benefits from a deep understanding of their underlying principles and limitations.


4.1. Fourier Transform Analysis


Fourier Transform analysis, a powerful technique rooted in signal processing, involves transforming an image from its spatial domain (where pixels are represented by their location) into the frequency domain.1 This transformation, often achieved using the Discrete Cosine Transform (DCT) in image processing contexts, allows for the analysis of the image's constituent frequencies. In the frequency domain, different image components are represented by their corresponding frequencies: high frequencies correspond to very small image components and fine details, such as sharp edges, intricate textures, or subtle noise. Conversely, mid to low frequencies represent the basic image structure, broader features, and smoother gradients.1
A key indicator of an upscaled image, particularly one processed with traditional deterministic resampling methods, is a noticeable absence or significant attenuation of high-frequency information.1 This phenomenon occurs because upscaling, being an interpolation process, does not genuinely create new fine details; it merely spreads the existing information across a larger pixel grid. Consequently, the high-frequency content, which represents these fine details, is either smoothed out or simply not generated.
Tools like resdet leverage this principle by specifically analyzing the frequency domain for unique patterns indicative of upscaling.4 Traditional resampling methods, such such as those used by tools like ImageMagick, introduce characteristic "zero-crossings" at specific offsets within the frequency domain.4 These zero-crossings act as a distinct signature, directly corresponding to the original resolution from which the image was upscaled.4
resdet identifies these specific inversions in the frequency spectrum to make its "best guess" about the original resolution.4
This method has a well-established history within certain online communities, having been utilized for over a decade, particularly for identifying upscales in Japanese animation.1 While specific external examples like the Anibin blog are referenced 1, the general principles are clearly articulated in the core research material.1
Despite its speed and ability to identify specific signatures, Fourier Transform analysis, particularly as implemented in resdet, has certain limitations and optimal conditions for performance. The tool performs optimally on clear, highly detailed images that are as close as possible to the original source.4 Its accuracy can be significantly compromised by the presence of compression artifacts (e.g., from highly compressed JPEGs or video codecs) and subsequent filtering applied to the image.4 For the most accurate results, detection should ideally be performed in the same colorspace in which the image was originally resized, such as linear RGB.4 When analyzing video content,
resdet is most effective with keyframes that have a low quantizer, and yuv4mpeg streams are preferred due to their superior preservation of chroma planes.4 A notable practical limitation is the potential for false positives when analyzing moderate to heavily compressed JPEG files, although the application of a deblocking filter can help mitigate this issue.4 Crucially, this method is fundamentally designed for traditional resampling techniques and will not work with newer neural network-based resizers due to their generative nature, which synthesizes new details rather than merely interpolating existing ones.4


4.2. Error Measurement (Trial and Error) Method


The Error Measurement method for native resolution detection, while potentially slower than Fourier Transform analysis, offers distinct advantages, including its relative simplicity of implementation and its capacity to identify instances where multiple native resolutions might exist within a single image.1 This method operates on an iterative "trial and error" principle.
The process begins by employing an inverse resizer to systematically downscale the source image to a wide range of possible resolutions.1 This range can be comprehensive (e.g., from 400p to 1079p) or a more focused, user-defined "sane range" based on common production resolutions.1 For each hypothetical native resolution tested, the resulting downscaled image is then immediately upscaled back to the original source resolution using the same resize kernel.1 Finally, the newly upscaled image (which has undergone a downscale-upscale cycle) is rigorously compared against the original source image.1
The core principle underpinning this method is that when the correct native resolution is identified, the error—defined as the absolute difference between the original image and the image that has been downscaled and then re-upscaled—will be significantly smaller and more distinct than the errors observed for all other tested resolutions.1 This pronounced "dip" or minimum in the error plot serves as a clear indicator of the true native resolution. An example implementation, provided for the Vapoursynth frame server, illustrates this process through a
get_error function. This function calculates the absolute difference between the source and the upscaled image (x - y abs) and then computes the luma_error, which is the average of the plane statistics of this difference, yielding a numerical value between 0 and 1.1
Practical tools such as getnative 9 and
GetFnative 11 are built upon this error measurement principle. These scripts automate the iterative process, generating plots of error versus resolution and suggesting potential native resolutions based on the identified error minima.2 The
getnative tool, for instance, allows users to specify a frame for analysis, a resize kernel (bicubic by default), and various parameters like b and c for bicubic, or taps for Lanczos, along with aspect ratio and height ranges.9 The
GetFnative script further extends this by supporting fractional native resolutions and handling scenarios involving cropping and letterboxing.11
The advantages of the Error Measurement method include its relative simplicity of implementation and its unique capacity to identify instances where multiple native resolutions might exist within a single image, a common occurrence in synthetic media.1 However, it is generally slower than Fourier transform methods, though still sufficiently fast for practical application (a few seconds per frame without multithreading), as typically only a few representative frames per video need to be analyzed.1 A critical prerequisite for accurate results is the careful selection of a "good frame" for analysis—one that is bright, exhibits minimal blur, contains few post-processed elements, and features clear lineart.2 This consistent advice across different methodologies highlights that the accuracy and reliability of native resolution detection are profoundly dependent on the "cleanliness" and fidelity of the source material. Any pre-existing noise, heavy compression artifacts (e.g., from highly compressed JPEGs), or complex post-processing (e.g., heavy visual effects, dynamic grain) will significantly degrade the effectiveness of these detection algorithms. This leads directly to the practical advice to "TRUST YOUR EYES OVER THIS SCRIPT!" 10 and "Trust your gut for 1080p" 12, as algorithmic output can become unreliable when faced with highly imperfect data.
A significant functional limitation of tools like getnative is their inability to automatically recognize content that is natively 1080p.9 This "blind spot" is inherent to the methodology. If a source is already native 1080p, there is no
upscale to reverse. When the tool attempts to downscale from 1080p (itself native) to a hypothetical lower resolution (e.g., 720p) and then upscale back to 1080p, it will always introduce some error due to the interpolation process. However, this error will not exhibit a distinct, sharp "dip" that points to a lower native resolution, because no such lower native resolution exists. Instead, the error might be uniformly low or lack a clear minimum, failing to provide the characteristic signature the algorithm looks for. This causal relationship between the method's design (seeking a lower native resolution) and the nature of truly native high-resolution content explains why human judgment remains crucial for confirming 1080p sources. The complementary nature of Fourier Transform analysis and Error Measurement methods suggests that an expert workflow would likely integrate both: FT-based tools for rapid initial screening of clean sources, and EM-based tools for precise identification and handling of complex scenarios.


5. The Mathematical Basis of Inverse Resizing (Descaling)


The ability to accurately reverse a deterministic resizing operation, known as descaling, is rooted in the principles of linear algebra. The process of resampling, whether upscaling or downscaling, can be precisely modeled as a linear equation. Understanding this mathematical framework is essential for appreciating the precision and limitations of descaling tools.


5.1. The Linear Equation Model: A∗x=b


The fundamental representation of resampling operations is expressed through the linear algebraic model: A∗x=b.1 This equation serves as the bedrock for understanding how deterministic image processing functions and, critically, how it can be inverted.
The components of this equation are defined as follows:
   * A (Resize Kernel Matrix): This is an n×m matrix that encapsulates the specific mathematical properties of the resize kernel that was originally used (e.g., Bicubic, Bilinear) and its associated parameters.1 In this context, 'm' represents the total number of pixels in the original, native-resolution image, while 'n' represents the total number of pixels in the upscaled (output) image. For upscaling operations, 'n' is typically greater than 'm', indicating that the output image has more pixels than the input. The matrix 'A' effectively describes how each pixel in the original image contributes to the formation of each pixel in the upscaled image, based on the interpolation function of the chosen kernel.
   * x (Original Pixels Vector): This is a vector containing the pixel values of the original, native-resolution image. It comprises 'm' elements, corresponding to the 'm' pixels of the source image.1 In the inverse problem of descaling, 'x' is the unknown variable that the process aims to recover. It represents the pristine, un-upscaled data.
   * b (Upscaled Pixels Vector): This is a vector containing the pixel values of the final, upscaled image. It consists of 'n' elements, corresponding to the 'n' pixels of the target image.1 This vector is considered known, as it constitutes the input image data that the descaling process receives. It is the observable, upscaled artifact from which the original 'x' must be inferred.
The challenge in descaling lies in solving this equation for 'x'. Directly solving for 'x' by inverting 'A' (i.e., x=A−1b) is generally not straightforward or even possible. This is because 'A' is typically a rectangular matrix (not square) when 'n' (output pixels) is not equal to 'm' (input pixels), and thus it does not possess a direct inverse in the traditional sense. Even if 'A' were square, its potentially very large size and its often ill-conditioned nature in real-world scenarios (due to noise, compression, or near-linear dependencies) would pose significant computational challenges for a direct inversion. Therefore, specialized numerical methods are required to find the most accurate approximation of 'x'.


5.2. Solving the Inverse Problem


To overcome the inherent difficulties of directly inverting the rectangular or ill-conditioned matrix 'A' in the equation A∗x=b, the problem is transformed into a solvable form using techniques from numerical linear algebra.
Transformation to ATAx=ATb (Normal Equations):
The standard approach to find the least-squares solution for 'x' in an overdetermined system (where there are more equations than unknowns, as is the case in upscaling where n>m) is to multiply both sides of the equation A∗x=b by the transpose of A, denoted as AT or A′.7 This yields the transformed equation:
ATAx=ATb.7 This transformation is a common technique used to convert a system of linear equations into a form where a unique or best-fit solution can be found.
The resulting matrix ATA is now a square, symmetric m×m matrix.7 Crucially, for well-behaved resizing kernels, this matrix is typically positive-definite, which ensures its invertibility and stability for numerical methods. The vector
ATb is a vector with 'm' elements.7 This transformed system, known as the normal equations, is now amenable to efficient numerical solution methods, even for very large image dimensions.
Detailed Explanation of LDLT Decomposition:
The Descale plugin, a key tool in Vapoursynth for inverse scaling, leverages LDLT decomposition to solve the system ATAx=ATb.7 LDLT decomposition is a specific method for factoring a symmetric matrix (like
ATA) into the product of a lower triangular matrix (L), a diagonal matrix (D), and the transpose of the lower triangular matrix (LT). The decomposition is expressed as ATA=LDLT.7
Both L and D are triangular matrices, meaning many of their elements are zero. This structure makes subsequent computational steps significantly simpler and more efficient.7 This decomposition is particularly well-suited for large, sparse, and banded symmetric matrices, which is characteristic of the
ATA matrix derived from image resizing kernels. The sparseness arises because each output pixel only depends on a limited number of input pixels, leading to many zero entries in the 'A' matrix. The "banded" nature means non-zero elements are concentrated around the main diagonal, further aiding computational efficiency.
Forward and Back Substitution for Pixel Restoration:
Once the LDLT decomposition of ATA is performed, the solution for 'x' (the original pixel values) is obtained through a two-step substitution process, effectively breaking down the complex matrix inversion into simpler, sequential solves:
   1. Forward Substitution: The first step involves solving the equation LDy=ATb for an intermediate vector 'y'. Since L is a lower triangular matrix and D is a diagonal matrix, this system can be solved efficiently by iterating through the equations from top to bottom, substituting already computed values of 'y' into subsequent equations. This process is computationally straightforward.
   2. Back Substitution: The final step involves solving the equation LTx=y for 'x'. Since LT is an upper triangular matrix (the transpose of a lower triangular matrix), this system can be solved efficiently by iterating through the equations from bottom to top, substituting already computed values of 'x' into preceding equations. This process yields the desired vector 'x', which represents the restored pixel values of the original, native-resolution image.7
This entire mathematical process, from formulating the normal equations to applying LDLT decomposition and subsequent substitutions, allows for a robust and computationally feasible method to invert deterministic upscaling operations, thereby restoring the original image data as accurately as possible within the constraints of real-world data imperfections.


6. Practical Implementation of Descaling


The theoretical understanding of native resolution detection and inverse resizing translates into practical application through a suite of specialized software tools and meticulous workflows. The Vapoursynth frame server, along with dedicated plugins and Python scripts, forms the backbone of modern descaling operations.


6.1. The Vapoursynth Ecosystem for Descaling


Vapoursynth is a powerful, open-source frame server designed for video processing. It provides a flexible and extensible environment where various filters and plugins can be chained together to perform complex video manipulations.6 Its Python scripting interface allows for highly customizable and precise control over every aspect of video processing, making it an ideal platform for sophisticated tasks like descaling.
At the core of descaling within Vapoursynth is the Descale plugin. This plugin is specifically designed to undo upscaling operations by applying the inverse mathematical models discussed previously.5
Descale supports a range of common resize kernels, including bicubic, bilinear, Lanczos, and spline upscales, which are frequently encountered in digitally produced anime content, often from resolutions such as 720p, 810p, 864p, or 900p.2 The plugin itself typically supports specific input formats like GrayS, RGBS, and YUV444PS, while its Python wrappers extend compatibility to various YUV subsampling formats, Gray, and RGB of every bitdepth.7
The Descale plugin's functionality is often accessed through convenient aliases provided by fvsfunc, such as fvf.Debilinear, simplifying its integration into Vapoursynth scripts.2 The plugin allows for precise control over kernel parameters, such as the
b and c values for bicubic kernels, and supports custom kernels through lambda functions for advanced users.8 The
Descale plugin also includes parameters for source cropping (src_left, src_top, src_width, src_height) and border handling, which are crucial for accurately aligning the descaling operation with the original content boundaries.7


6.2. Key Tools and Scripts for Detection and Application


Several Python scripts complement the Descale plugin, automating the complex processes of native resolution detection and facilitating the application of descaling.
   * getnative.py: This is a widely used Python script designed to find the native resolution(s) of upscaled video material, primarily anime.9 It implements the Error Measurement method, analyzing a given video frame to determine the original resolution before upscaling.9 The script generates a graph where the X-axis represents resolutions and the Y-axis displays the relative error. A clear spike showing a low relative error indicates the native resolution.12 Users can specify a particular frame for analysis, choose a resize kernel (Mitchell-Netravali bicubic by default), and configure parameters like
b and c for bicubic, taps for Lanczos, aspect ratio, and height range.9 The script requires Python 3.6+,
matplotlib, Vapoursynth R45+, descale, and a source filter like ffms2, lsmash, or imwri.9 For optimal results,
getnative.py recommends selecting a high-quality, bright frame with minimal blur, few post-processed elements, and clear lineart.2 A significant limitation is its inability to automatically recognize 1080p native productions, requiring user judgment for such cases.9
   * GetFnative: This script, a variant of getnative, focuses on finding native fractional resolutions of upscaled material, especially anime.11 It extends the "naive search" approach to find fractional
src_height values within a specified interval and step length.11
GetFnative can handle scenarios where base_height and base_width are unknown, often only requiring their parity for descaling.11 It also supports descaling in only width or height to aid in identifying parity individually, and can acquire cropping parameters for descaling.11 A "quick" version,
getfnativeq, checks preset values with limited options for faster analysis.11
   * getfscaler: Described as an unofficial "companion" script to getfnative, getfscaler is a rewrite of the original getscaler tool, providing fractional descaling support using vsjet packages.10 It employs more robust JET tooling, defaults to kernels found in professional software plus Point, and includes post-filtering methods to reduce errors from dirty edges and dithering.10 It supports fractional descaling, cross-converted video, and more image types without relying on
ffms2.10 The script provides warnings and information, and offers optional features like one-dimensional scaling and debug output.10 A crucial warning accompanying
getfscaler is that its results are not conclusive on their own and may be inaccurate, emphasizing the need to "TRUST YOUR EYES OVER THIS SCRIPT!".10
   * resdet: This command-line tool and C library detects upscaling and determines original resolution by analyzing the image's frequency domain using the Discrete Cosine Transform.4 It identifies "zero-crossings" unique to traditional resampling methods, which correspond to the original resolution.4
resdet works best on clear, detailed images and is sensitive to compression artifacts and filtering, performing optimally in the original resize colorspace (e.g., linear RGB).4 It is limited to traditional methods and will not work with neural network-based resizers.4


6.3. The Descaling Workflow: From Detection to Verification


The practical descaling process involves a systematic workflow, from identifying the native resolution and kernel to verifying the quality of the inverse scale.
      1. Preparation and Frame Selection: Before initiating descaling, two primary tools are needed: getnative.py for resolution detection and BluBb-mADe’s descale for the actual descaling process.12 The first critical step is to select a "good frame" for analysis. This should be a high-quality, bright frame with minimal blur, few post-processed elements, and clear lineart.2 Examples of "bad" frames include dark scenes, those with heavy effects, or dynamic grain, as these can obscure the underlying scaling artifacts and lead to inaccurate detection.12 If the frame contains letterboxing, it must be cropped out prior to analysis to prevent distorted graphs.12 This rigorous frame selection is vital because the accuracy of native resolution detection is profoundly dependent on the "cleanliness" and fidelity of the source material. Any pre-existing noise, heavy compression artifacts, or complex post-processing will significantly degrade the effectiveness of these detection algorithms.
      2. Finding the Native Resolution: The selected frame is then processed using getnative.py.12 The script generates a graph plotting resolution against relative error. The objective is to identify a clear, distinct spike showing a low relative error, which indicates the native resolution.12 For instance, a graph might clearly point to 878p as the native resolution.12 It is crucial to watch for "bad" graphs that are unclear or lack a single, distinct spike; in such cases, descaling is too risky and can be destructive.12 As noted,
getnative.py cannot find native 1080p elements, requiring encoder judgment for such sources.9 The error measurement method, which
getnative employs, works by attempting to reverse a hypothetical upscale. If the source is already native 1080p, there's no upscale to reverse, and the process of downscaling and re-upscaling will always introduce some error, but without the characteristic "dip" that signals a lower native resolution. This explains why human judgment remains crucial for confirming 1080p sources.
      3. Determining the Upscale Kernel: Once a candidate native resolution is identified, the next critical step is to determine the specific upscale kernel used by the studio or publisher. While getnative.py defaults to Mitchell-Netravali (a bicubic variant), studios may use other kernels.12 The most reliable method is a comparative trial-and-error approach: testing a variety of common kernels (e.g., Lanczos, Spline16, Spline36, Bilinear, various Bicubic settings) and closely examining the results for scaling-related artifacts like haloing or ringing.2 For bicubic kernels, values often follow patterns like
b+2c=1, b=0,c=X, or b=1,c=0.12
      4. Verification through Rescale Test: To confirm the correct kernel and resolution, a rescale test is performed. This involves descaling the source video using the identified kernel and resolution, then immediately upscaling the descaled video back to the original source dimensions using the same kernel.2 The chroma from the original source is typically merged with the rescaled image, as chroma often has a lower resolution and cannot be descaled effectively.2 The original source and the rescaled image are then interleaved for direct comparison.2 The expectation is that the lineart in the rescaled image should be "practically identical" to the original, with no additional haloing or aliasing introduced.2 Zooming in at 4x magnification or higher is recommended for this comparison to discern subtle differences.12 An incorrect descale and rescale will show "lots more artifacts".2 This verification step is crucial because, as noted, perfect descaling is unattainable due to the non-lossless nature of consumer sources like Blu-rays, which always introduce small, albeit often negligible, differences from the original masters.12
      5. Dealing with Mixed Resolutions and Challenges: Many videos contain elements at different resolutions within the same frame, such as a background in 900p and a character in 720p, or 1080p credits and overlays within a lower-resolution video.2 Descaling the entire frame in such cases can introduce haloing artifacts to the elements that are already native 1080p.2 To address this,
fvsfunc provides DescaleM functions (e.g., fvf.DebilinearM), which mask elements at different resolutions and scale them using a standard spline36 resize.2 While slower, these functions can be applied selectively to frames containing such elements.2 Wrappers like
inverse_scale from kagefunc can also automate this masking.12 Examining the mask generated by these wrappers is important; if the mask catches "other stuff" beyond the intended elements, it may indicate an incorrect kernel or that the frames are truly native 1080p and should not be descaled.12 In rare cases where resolution or kernel changes frame-by-frame,
lvsfunc.scale.descale attempts automatic height detection, though manual intervention is often ideal.2 If a scene continues to exhibit issues after descaling, potential solutions include trying a different kernel/values, applying an
Eedi3 filter (a slow but effective anti-aliasing filter for bad lineart), or, in some cases, deciding not to descale the clip at all.12 If working with a poorly descaled encode from another source, finding a different encode is often the best solution.12


7. Conclusions and Recommendations


The comprehensive analysis of native resolution restoration reveals it to be a sophisticated and indispensable technique within digital media processing, driven by both economic imperatives and the pursuit of visual fidelity. The practice of descaling, which involves reversing upscaling operations, addresses a pervasive industry challenge where content is often upscaled suboptimally to meet contemporary resolution standards for distribution.
The primary motivations for descaling are clear: significant reductions in file size and bandwidth consumption, leading to substantial cost savings for distributors and improved efficiency for archivists. Furthermore, descaling optimizes computational workflows by allowing subsequent processing and compression to occur on smaller, native-resolution data. Critically, it enables a two-stage quality enhancement process: undoing a "cheap" initial upscale to provide a clean, native-resolution base, which can then be re-upscaled using superior algorithms to achieve a subjectively sharper output with fewer artifacts. This strategic re-upscaling, whether performed by the encoder or the end-user's playback system, represents a powerful method for reclaiming and enhancing the intrinsic quality of digital media.
The theoretical foundation of descaling rests on the deterministic nature of traditional resizing kernels, allowing their inversion through linear algebraic methods such as LDLT decomposition. This mathematical elegance, however, encounters practical limitations due to real-world data imperfections like compression artifacts and noise, rendering perfect descaling "near-lossless" rather than truly lossless. This distinction is crucial for managing expectations and guiding practical application.
Two principal methodologies, Fourier Transform analysis and Error Measurement, offer complementary approaches to native resolution detection. Fourier Transform methods, exemplified by resdet, are fast and effective for clean sources, identifying unique frequency signatures of upscaling. Error Measurement methods, implemented in tools like getnative and GetFnative, are more robust for precise resolution identification, including fractional values, and can detect multiple native resolutions within a single frame, albeit at a slower pace. The consistent emphasis across both methodologies on the "cleanliness" of the source material highlights a fundamental truth: the accuracy of detection is highly sensitive to pre-existing noise, compression artifacts, and complex post-processing. This underscores why human judgment remains indispensable, particularly for confirming truly native high-resolution content (e.g., 1080p), which algorithmic tools may struggle to identify due to their inherent design.
Recommendations for Optimal Native Resolution Restoration:
         1. Prioritize Source Quality: Always begin with the cleanest possible source material. Minimize pre-existing compression artifacts, noise, and heavy post-processing effects, as these significantly degrade the accuracy of native resolution detection and the quality of the descaling process. If multiple encodes are available, select the one with the highest fidelity.
         2. Employ Complementary Detection Methodologies: Do not rely on a single detection tool or method. Utilize both Fourier Transform-based tools (like resdet for quick initial assessments on clean sources) and Error Measurement-based tools (like getnative or GetFnative for precise, iterative analysis and fractional resolution detection). Cross-reference results to build confidence in the identified native resolution.
         3. Meticulous Frame Selection: When using Error Measurement tools, carefully select "good frames" for analysis. These frames should be bright, have clear lineart, and be free from excessive blur, dynamic grain, or complex visual effects. Crop out any letterboxing before analysis to avoid skewed results.
         4. Iterative Kernel Identification: The precise identification of the original upscale kernel is paramount. This often requires a systematic, comparative trial-and-error approach, testing various common kernels (Bilinear, Bicubic with different b/c values, Lanczos, Spline) and visually inspecting the results for the minimization of artifacts like haloing and ringing. Leverage Vapoursynth scripts that allow for quick descale-rescale comparisons.
         5. Rigorous Verification: Always perform a rescale test to verify the accuracy of the descaling. Descape the content with the identified parameters, then upscale it back to the original dimensions using the same kernel. Interleave the original and the processed frames for pixel-perfect comparison, ensuring lineart is virtually identical and no new artifacts are introduced.
         6. Strategic Handling of Mixed Resolutions: Be prepared for content with mixed native resolutions (e.g., 1080p credits on a 720p animated background). Utilize masking functions (e.g., DescaleM in fvsfunc or inverse_scale in kagefunc) to apply descaling only to the relevant lower-resolution elements, while preserving higher-resolution overlays or graphics.
         7. Acknowledge Limitations: Understand that deterministic descaling cannot reverse upscales performed by generative neural networks (e.g., waifu2x, RAISR) due to their synthetic detail creation. For truly native 1080p content, rely on informed judgment, as error measurement tools may not provide a distinct "dip."
         8. Prioritize User Experience: Consider the end-user's playback environment. Releasing content in its native resolution allows advanced video players (e.g., MadVR) to perform high-quality, real-time upscaling, potentially delivering a superior viewing experience than a pre-upscaled release.
By adhering to these principles and leveraging the sophisticated tools available, media professionals can effectively restore the native resolution of digital content, leading to optimized storage, more efficient processing, and ultimately, a higher-quality visual experience for consumers and a more faithful preservation of digital assets.
Works cited
         1. accessed January 1, 1970,
         2. Descaling and Rescaling - Advanced Encoding Guide - GitLab, accessed August 11, 2025, https://silentaperture.gitlab.io/mdbook-guide/filtering/descaling.html
         3. kageru.moe, accessed August 11, 2025, https://blog.kageru.moe/legacy/resolutions.html
         4. 0x09/resdet: Detect source resolution of upscaled images - GitHub, accessed August 11, 2025, https://github.com/0x09/resdet
         5. Deb Multimedia Packages::vapoursynth-descale, accessed August 11, 2025, https://deb-multimedia.org/dists/bookworm/main/binary-arm64/package/vapoursynth-descale
         6. Welcome to VapourSynth's documentation!, accessed August 11, 2025, https://www.vapoursynth.com/doc/
         7. DJATOMs-archive/vapoursynth-descale - GitHub, accessed August 11, 2025, https://github.com/DJATOMs-archive/vapoursynth-descale
         8. Irrational-Encoding-Wizardry/descale: VapourSynth plugin to undo upscaling - GitHub, accessed August 11, 2025, https://github.com/Irrational-Encoding-Wizardry/descale
         9. Infiziert90/getnative: Find the native resolution(s) of ... - GitHub, accessed August 11, 2025, https://github.com/Infiziert90/getnative
         10. Jaded-Encoding-Thaumaturgy/getfscaler: getscaler with ... - GitHub, accessed August 11, 2025, https://github.com/Jaded-Encoding-Thaumaturgy/getfscaler
         11. YomikoR/GetFnative: A script that help find the native ... - GitHub, accessed August 11, 2025, https://github.com/YomikoR/GetFnative
         12. Descaling - Fansubbing Guide, accessed August 11, 2025, https://guide.encode.moe/encoding/descaling.html